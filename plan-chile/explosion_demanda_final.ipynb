{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd085eb9",
   "metadata": {},
   "source": [
    "# Carga de datos para explosion e ingesta en SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8be96f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Ruta del archivo\n",
    "archivo = r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\ZMM0164 13.05.2025.xlsx\"\n",
    "\n",
    "# Cargar hoja con openpyxl\n",
    "wb = load_workbook(archivo, read_only=True)\n",
    "ws = wb.active\n",
    "\n",
    "# Buscar la fila donde aparece \"Material\"\n",
    "for i, row in enumerate(ws.iter_rows(values_only=True)):\n",
    "    if row and any(str(cell).strip().lower() == \"material\" for cell in row):\n",
    "        header_row = i\n",
    "        break\n",
    "\n",
    "# Leer el archivo con pandas desde esa fila\n",
    "ZMM0164 = pd.read_excel(archivo, engine=\"openpyxl\", skiprows=header_row)\n",
    "\n",
    "# Elimina columnas sin nombre (NaN o \"Unnamed\")\n",
    "ZMM0164 = ZMM0164.loc[:, ~ZMM0164.columns.to_series().astype(str).str.contains(\"^Unnamed\")]\n",
    "\n",
    "# Elimina la primera fila si todos sus valores son nulos\n",
    "if ZMM0164.iloc[0].isnull().all():\n",
    "    ZMM0164 = ZMM0164.iloc[1:]\n",
    "\n",
    "# Resetear índice después de limpieza (opcional)\n",
    "ZMM0164.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Convertir tipo de datos\n",
    "ZMM0164[\"Material\"] = ZMM0164[\"Material\"].astype(\"Int64\")\n",
    "ZMM0164[\"NºMaterial antiguo\"] = ZMM0164[\"NºMaterial antiguo\"].astype(\"Int64\")\n",
    "\n",
    "# Obtener transacción de kits\n",
    "ZQUEPP22N = pd.read_excel(r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\ZQUEPP22N 13.5.2025.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b09fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la tabla final a construir, con los productos \"explotados\"\n",
    "explosion = pd.DataFrame([])\n",
    "\n",
    "# Inicialización del dataframe\n",
    "explosion['cv_id'] = ZMM0164['NºMaterial antiguo']\n",
    "explosion['cv_prod'] = ZMM0164['NºMaterial antiguo']\n",
    "explosion['cm_kit'] = ZMM0164['Material']\n",
    "explosion['cm_prod'] = ZMM0164['Material']\n",
    "explosion['tipo_prod'] = ZMM0164['TpMt']\n",
    "explosion['producto'] = ZMM0164['Texto breve de material']\n",
    "explosion['marca'] = ZMM0164['Marca']\n",
    "explosion['cantidad'] = ''\n",
    "\n",
    "df_final = explosion.copy()\n",
    "\n",
    "iter = 1\n",
    "\n",
    "while True:\n",
    "    if iter <= 2:\n",
    "        # Filtrar registros ZEST\n",
    "        df_zest = df_final[df_final['tipo_prod'] == 'ZEST'][['cm_kit', 'cantidad']].drop_duplicates()\n",
    "        print(f'Cantidad de ZEST pendientes: {df_zest['cm_kit'].count()}')\n",
    "\n",
    "        if df_zest.empty:\n",
    "            break  # Nada más que desagregar\n",
    "\n",
    "        # Sí cantidad = '', convertir a 1\n",
    "        df_zest['cantidad'] = df_zest['cantidad'].replace(['', ' '], None)\n",
    "        df_zest['cantidad'] = df_zest['cantidad'].fillna(1).astype(int)\n",
    "\n",
    "        # PASO1: Hacer el left join y seleccionar solo los campos requeridos\n",
    "        df_joined = df_zest.merge(\n",
    "            ZQUEPP22N[['Material', 'LisTéc.', 'Componente', 'Qtd.']],\n",
    "            how='left',\n",
    "            left_on='cm_kit',\n",
    "            right_on='Material'\n",
    "        )[['cm_kit', 'LisTéc.', 'Componente', 'Qtd.', 'cantidad']]    \n",
    "\n",
    "        # Calcular Qtd. x Cantidad \n",
    "        df_joined['cantidad2'] = df_joined['Qtd.']*df_joined['cantidad']\n",
    "        df_joined = df_joined.drop(columns = ['Qtd.', 'cantidad'])\n",
    "        df_joined = df_joined.rename(columns = {'cantidad2': 'cantidad'})  \n",
    "\n",
    "        # Paso 2: Renombrar columnas para nuevo esquema\n",
    "        df_joined = df_joined.rename(columns={\n",
    "             'LisTéc.': 'cv_prod',\n",
    "            'Componente': 'cm_prod',\n",
    "            'Qtd.': 'cantidad'})\n",
    "\n",
    "        df_joined[\"cv_prod\"] = df_joined[\"cv_prod\"].astype(\"Int64\")\n",
    "        df_joined[\"cm_prod\"] = df_joined[\"cm_prod\"].astype(\"Int64\")\n",
    "\n",
    "        # PASO 3: Hacemos left join con explosion en cm_kit\n",
    "        df_merged = df_joined.merge(\n",
    "            explosion,\n",
    "            how='left',\n",
    "            on='cm_kit',\n",
    "            suffixes=('_joined', '_explosion')\n",
    "        )\n",
    "\n",
    "        # Reemplazamos los valores de 'cv_prod' y 'cm_prod' de explosion con los de df_joined\n",
    "        df_merged['cv_prod'] = df_merged['cv_prod_joined']\n",
    "        df_merged['cm_prod'] = df_merged['cm_prod_joined']\n",
    "        df_merged['cantidad'] = df_merged['cantidad_joined']\n",
    "\n",
    "        # Eliminamos las columnas que ya no necesitamos\n",
    "        df_resultado = df_merged.drop(columns=['cv_prod_joined', 'cm_prod_joined'])\n",
    "\n",
    "        # Opcionalmente, si quieres reordenar las columnas como en explosion\n",
    "        df_resultado = df_resultado[explosion.columns]\n",
    "\n",
    "        # Paso 4: Preparar la tabla de referencia desde explosion\n",
    "        referencia = explosion[['cm_kit', 'cv_prod', 'tipo_prod', 'producto', 'marca']].drop_duplicates()\n",
    "        referencia = referencia.rename(columns={'cm_kit': 'cm_prod'})  # porque vamos a unir por cm_prod\n",
    "\n",
    "        # Paso 5: Eliminar los campos que vamos a reemplazar, si ya existen\n",
    "        df_base = df_resultado.drop(columns=['cv_prod', 'tipo_prod', 'producto', 'marca'], errors='ignore')\n",
    "\n",
    "        # Paso 6: Merge con la referencia para traer los valores actualizados\n",
    "        df_actualizado = df_base.merge(\n",
    "            referencia,\n",
    "            how='left',\n",
    "            on='cm_prod'\n",
    "        )\n",
    "\n",
    "        df_final = pd.concat([df_final[df_final['tipo_prod'] != 'ZEST' ], df_actualizado], ignore_index=True).drop_duplicates()\n",
    "\n",
    "        df_final['cantidad'] = (\n",
    "            df_final['cantidad']\n",
    "            .replace(r'^\\s*$', None, regex=True)  # Reemplaza cadenas vacías o con espacios por None\n",
    "            .fillna(1)                            # Rellena los None con 1\n",
    "            .astype(int)                          # Convierte a entero\n",
    "        )\n",
    "\n",
    "        iter = iter + 1\n",
    "\n",
    "    else:\n",
    "        df_zest = df_final[df_final['tipo_prod'] == 'ZEST'][['cv_id', 'cm_prod', 'cantidad']]            \n",
    "        print(f'Cantidad de ZEST pendientes: {df_zest['cm_prod'].count()}')\n",
    "\n",
    "        if df_zest.empty:\n",
    "            break  # Nada más que desagregar\n",
    "\n",
    "        # Sí cantidad = '', convertir a 1\n",
    "        df_zest['cantidad'] = df_zest['cantidad'].replace(['', ' '], None)\n",
    "        df_zest['cantidad'] = df_zest['cantidad'].fillna(1).astype(int)\n",
    "\n",
    "        # PASO1: Hacer el left join y seleccionar solo los campos requeridos\n",
    "        df_joined = df_zest.merge(\n",
    "            ZQUEPP22N[['Material', 'LisTéc.', 'Componente', 'Qtd.']],\n",
    "            how='left',\n",
    "            left_on='cm_prod',\n",
    "            right_on='Material'\n",
    "        )[['cv_id', 'cm_prod', 'LisTéc.', 'Componente', 'Qtd.', 'cantidad']]    \n",
    "\n",
    "        # Calcular Qtd. x Cantidad \n",
    "        df_joined['cantidad2'] = df_joined['Qtd.']*df_joined['cantidad']\n",
    "        df_joined = df_joined.drop(columns = ['Qtd.', 'cantidad'])\n",
    "        df_joined = df_joined.rename(columns = {'cantidad2': 'cantidad'})  \n",
    "    \n",
    "        # Paso 2: Renombrar columnas para nuevo esquema\n",
    "        df_joined = df_joined.rename(columns={\n",
    "            'cm_prod': 'cm_kit',\n",
    "            'LisTéc.': 'cv_prod',\n",
    "            'Componente': 'cm_prod',\n",
    "            'Qtd.': 'cantidad'})\n",
    "\n",
    "        df_joined[\"cv_prod\"] = df_joined[\"cv_prod\"].astype(\"Int64\")\n",
    "        df_joined[\"cm_prod\"] = df_joined[\"cm_prod\"].astype(\"Int64\")\n",
    "\n",
    "        # PASO 3: Hacemos left join con explosion en cm_kit\n",
    "        df_merged = df_joined.merge(\n",
    "            explosion,\n",
    "            how='left',\n",
    "            on='cm_kit',\n",
    "            suffixes=('_joined', '_explosion')\n",
    "        )\n",
    "\n",
    "        # Reemplazamos los valores de 'cv_prod' y 'cm_prod' de explosion con los de df_joined\n",
    "        df_merged['cv_prod'] = df_merged['cv_prod_joined']\n",
    "        df_merged['cm_prod'] = df_merged['cm_prod_joined']\n",
    "        df_merged['cv_id'] = df_merged['cv_id_joined']\n",
    "        df_merged['cantidad'] = df_merged['cantidad_joined']\n",
    "\n",
    "        # Eliminamos las columnas que ya no necesitamos\n",
    "        df_resultado = df_merged.drop(columns=['cv_prod_joined', 'cm_prod_joined', 'cv_id_joined'])\n",
    "\n",
    "        # Opcionalmente, si quieres reordenar las columnas como en explosion\n",
    "        df_resultado = df_resultado[explosion.columns]\n",
    "\n",
    "        # Paso 4: Preparar la tabla de referencia desde explosion\n",
    "        referencia = explosion[['cm_kit', 'cv_prod', 'tipo_prod', 'producto', 'marca']].drop_duplicates()\n",
    "        referencia = referencia.rename(columns={'cm_kit': 'cm_prod'})  # porque vamos a unir por cm_prod\n",
    "\n",
    "        # Paso 5: Eliminar los campos que vamos a reemplazar, si ya existen\n",
    "        df_base = df_resultado.drop(columns=['cv_prod', 'tipo_prod', 'producto', 'marca'], errors='ignore')\n",
    "\n",
    "        # Paso 6: Merge con la referencia para traer los valores actualizados\n",
    "        df_actualizado = df_base.merge(\n",
    "            referencia,\n",
    "            how='left',\n",
    "            on='cm_prod'\n",
    "        )\n",
    "\n",
    "        df_final = pd.concat([df_final[df_final['tipo_prod'] != 'ZEST' ], df_actualizado], ignore_index=True).drop_duplicates()\n",
    "        print(f'Cantidad de prod resultantes: {df_final['cm_kit'].count()}')\n",
    "\n",
    "        df_final['cantidad'] = (\n",
    "            df_final['cantidad']\n",
    "            .replace(r'^\\s*$', None, regex=True)  # Reemplaza cadenas vacías o con espacios por None\n",
    "            .fillna(1)                            # Rellena los None con 1\n",
    "            .astype(int)                          # Convierte a entero\n",
    "        )\n",
    "        \n",
    "df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2c006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingesta en SQL Server 2019\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine import URL\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Obtener credenciales SQL Server\n",
    "# Path to the file\n",
    "file_path = r\"C:\\Users\\Spider Build\\Downloads\\SQL2019.txt\"\n",
    "\n",
    "# Function to extract username and password\n",
    "def extract_credentials(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    username_match = re.search(r'username=\"([^\"]+)\"', content)\n",
    "    password_match = re.search(r'password=\"([^\"]+)\"', content)\n",
    "    \n",
    "    if username_match and password_match:\n",
    "        username = username_match.group(1)\n",
    "        password = password_match.group(1)\n",
    "        return username , password\n",
    "    else:\n",
    "        return \"Username or password not found in the file.\"\n",
    "\n",
    "# Llamar la función de credenciales\n",
    "username_sql, password_sql = extract_credentials(file_path)\n",
    "\n",
    "# Función para insertar en SQL Server\n",
    "def insertar_df_sql(\n",
    "    df: pd.DataFrame,\n",
    "    database_name: str,\n",
    "    schema_name: str,\n",
    "    table_name: str,\n",
    "    server_address: str = \"10.156.16.46\\\\SQL2022\",\n",
    "    driver_name: str = \"ODBC Driver 17 for SQL Server\",\n",
    "    username_sql: str = None, # Puedes pasarlo como parámetro o cargarlo de forma segura\n",
    "    password_sql: str = None, # Puedes pasarlo como parámetro o cargarlo de forma segura\n",
    "    method: str = None, # Puede haber problemas con multi, utilizar None\n",
    "    if_exists_option: str = 'append',\n",
    "    chunk_size: int = 1000 # Un chunksize más grande suele ser más eficiente\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Inserta un DataFrame de pandas en una tabla de SQL Server.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): El DataFrame a insertar.\n",
    "        database_name (str): El nombre de la base de datos (ej. \"CustomerCare\").\n",
    "        schema_name (str): El nombre del esquema (ej. \"customer_care\").\n",
    "        table_name (str): El nombre de la tabla (ej. \"reclamos\").\n",
    "        server_address (str): Dirección del servidor SQL Server (por defecto \"10.156.16.46\\\\SQL2022\").\n",
    "        driver_name (str): Nombre del driver ODBC (por defecto \"ODBC Driver 17 for SQL Server\").\n",
    "        username_sql (str, optional): Nombre de usuario para la conexión SQL. Si no se provee,\n",
    "                                      la función buscará en variables de entorno o usará autenticación integrada.\n",
    "        password_sql (str, optional): Contraseña para la conexión SQL.\n",
    "        if_exists_option (str): Cómo manejar la tabla si ya existe ('fail', 'replace', 'append').\n",
    "                                Por defecto 'append'.\n",
    "        chunk_size (int): Número de filas a escribir por lote. Por defecto 1000.\n",
    "    \"\"\"\n",
    "    # --- Configuración de autenticación (ejemplo) ---\n",
    "    # Construcción de la URL de conexión\n",
    "    connection_url = URL.create(\n",
    "        \"mssql+pyodbc\",\n",
    "        username=username_sql, # Podría ser None si usas autenticación integrada o variables de entorno\n",
    "        password=password_sql, # Podría ser None\n",
    "        host=server_address,\n",
    "        database=database_name,\n",
    "        query={\n",
    "            \"driver\": driver_name,\n",
    "            #\"TrustServerCertificate\": \"yes\", # Es común necesitar esto si no tienes certificados configurados\n",
    "            # \"authentication\": \"ActiveDirectoryIntegrated\", # Descomenta si usas esta autenticación\n",
    "        },\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Crear el motor de conexión\n",
    "        engine = create_engine(connection_url)\n",
    "\n",
    "        with engine.begin() as connection:\n",
    "            start_time = time.time()\n",
    "            df.to_sql(\n",
    "                name=table_name,\n",
    "                schema=schema_name,\n",
    "                con=connection,\n",
    "                method=method,\n",
    "                chunksize=chunk_size,\n",
    "                if_exists=if_exists_option,\n",
    "                index=False, # Generalmente no queremos el índice de pandas como columna en SQL\n",
    "            )\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"{len(df)} Datos insertados en '{database_name}.{schema_name}.{table_name}' exitosamente en {elapsed_time:.2f} segundos.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al insertar datos en la base de datos: {e}\")\n",
    "\n",
    "# Ver posibilidades de mejora del method al usar None\n",
    "#def custom_insert(table, conn, keys, data_iter):\n",
    "#    from sqlalchemy.dialects.mssql import insert\n",
    "#    values_list = [dict(zip(keys, row)) for row in data_iter]\n",
    "#    stmt = insert(table).values(values_list)\n",
    "#    conn.execute(stmt)\n",
    "\n",
    "#df.to_sql(\n",
    "#    name=table_name,\n",
    "#    schema=schema_name,\n",
    "#    con=connection,\n",
    "#    method=custom_insert,\n",
    "#    chunksize=chunk_size,\n",
    "#    if_exists=if_exists_option,\n",
    "#    index=False\n",
    "#)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63cf342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación de datos\n",
    "df_processed = df_final.copy() \n",
    "\n",
    "df_processed['cv_id'] = df_processed['cv_id'].astype('Int64')\n",
    "df_processed['cv_prod'] = df_processed['cv_prod'].astype('Int64')\n",
    "df_processed['cm_kit'] = df_processed['cm_kit'].astype('Int64')\n",
    "df_processed['cm_prod'] = df_processed['cm_prod'].astype('Int64')\n",
    "df_processed['tipo_prod'] = df_processed['tipo_prod'].astype(str)\n",
    "df_processed['producto'] = df_processed['producto'].astype(str)\n",
    "df_processed['marca'] = df_processed['marca'].astype(str)\n",
    "df_processed['cantidad'] = df_processed['cantidad'].astype('Int16')\n",
    "\n",
    "# Insertar tabla productos de de la explosión de panel digital\n",
    "insertar_df_sql(\n",
    "        df=df_processed,\n",
    "        server_address = \"10.156.16.45\\SQL2019\",\n",
    "        database_name=\"OyLCL\",\n",
    "        schema_name=\"plan_chile\",\n",
    "        table_name=\"productos\", # Usa un nombre de tabla temporal para pruebas\n",
    "        username_sql=username_sql,\n",
    "        password_sql=password_sql,\n",
    "        method = None,\n",
    "        if_exists_option='replace' # 'replace' para crearla de cero cada vez en pruebas\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86114f1f",
   "metadata": {},
   "source": [
    "# PROCESAMIENTO RETAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c019b61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar explosión demanda de RETAIL \n",
    "\n",
    "# Se crea función que carga un .xlsx a dataframe en Pandas, con try & catch.\n",
    "def crear_dataframe(ruta_archivo):\n",
    "    \"\"\"\n",
    "    Crea un DataFrame de pandas a partir de un archivo Excel.\n",
    "\n",
    "    Args:\n",
    "        ruta_archivo (str): La ruta completa del archivo Excel.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: El DataFrame 'productos' creado a partir del archivo,\n",
    "                      o None si ocurre un error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        productos = pd.read_excel(ruta_archivo)\n",
    "        print(\"DataFrame creado exitosamente.\")\n",
    "        return productos\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: El archivo no se encontró en la ruta especificada: {ruta_archivo}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error al leer el archivo Excel: {e}\")\n",
    "        return None\n",
    "    \n",
    "path_retail = r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\Demanda Retail.xlsx\"\n",
    "\n",
    "exp_retail = crear_dataframe(path_retail)\n",
    "exp_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c99f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cruzar dataframe exp_retail con los productos a nivel de Código = cv\n",
    "import numpy as np\n",
    "\n",
    "# 1. Aislar número\n",
    "exp_retail['Código'] = exp_retail['Código'].str.replace('P', '').astype(np.int64)\n",
    "exp_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b0c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Realizar left join con la tabla productos de plan_chile\n",
    "# Método para obtener df desde SQL Server\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine import URL\n",
    "import time\n",
    "\n",
    "def sql_to_df(\n",
    "    database_name: str,\n",
    "    schema_name: str,\n",
    "    table_name: str = None,\n",
    "    query: str = None,\n",
    "    server_address: str = \"10.156.16.45\\SQL2019\",\n",
    "    driver_name: str = \"ODBC Driver 17 for SQL Server\",\n",
    "    username_sql: str = None,\n",
    "    password_sql: str = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lee datos de una tabla de SQL Server o ejecuta una consulta SQL y los carga en un DataFrame de pandas.\n",
    "\n",
    "    Este método establece una conexión con SQL Server utilizando SQLAlchemy y pyodbc.\n",
    "    Permite tanto la lectura de una tabla completa especificando 'table_name'\n",
    "    como la ejecución de una consulta SQL personalizada mediante el parámetro 'query'.\n",
    "    Maneja la construcción de la URL de conexión y proporciona un control básico de errores.\n",
    "\n",
    "    Args:\n",
    "        database_name (str): El nombre de la base de datos (ej. \"CustomerCare\").\n",
    "        schema_name (str): El nombre del esquema (ej. \"customer_care\").\n",
    "        table_name (str, optional): El nombre de la tabla a leer. Se requiere si 'query' no se proporciona.\n",
    "                                     Si se usa, la función construirá un SELECT * FROM.\n",
    "        query (str, optional): La consulta SQL a ejecutar. Se requiere si 'table_name' no se proporciona.\n",
    "                                Se ejecutará la consulta directamente.\n",
    "        server_address (str): Dirección del servidor SQL Server (por defecto \"10.156.16.46\\\\SQL2022\").\n",
    "        driver_name (str): Nombre del driver ODBC (por defecto \"ODBC Driver 17 for SQL Server\").\n",
    "        username_sql (str, optional): Nombre de usuario para la conexión SQL. Si es None,\n",
    "                                      se intentará la autenticación integrada o se usarán variables de entorno.\n",
    "        password_sql (str, optional): Contraseña para la conexión SQL. Si es None,\n",
    "                                      se intentará la autenticación integrada o se usarán variables de entorno.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un DataFrame de pandas con los datos leídos de SQL Server.\n",
    "                      Retorna un DataFrame vacío si ocurre un error o no se encuentran datos.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Si no se proporciona ni 'table_name' ni 'query'.\n",
    "    \"\"\"\n",
    "    if table_name is None and query is None:\n",
    "        raise ValueError(\"Debes proporcionar 'table_name' o 'query'.\")\n",
    "\n",
    "    connection_url = URL.create(\n",
    "        \"mssql+pyodbc\",\n",
    "        username=username_sql,\n",
    "        password=password_sql,\n",
    "        host=server_address,\n",
    "        database=database_name,\n",
    "        query={\n",
    "            \"driver\": driver_name,\n",
    "            # \"TrustServerCertificate\": \"yes\", # Descomentar si es necesario para certificados autofirmados/no verificados\n",
    "            # \"authentication\": \"ActiveDirectoryIntegrated\", # Descomentar si usas esta autenticación\n",
    "        },\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    try:\n",
    "        engine = create_engine(connection_url)\n",
    "\n",
    "        start_time = time.time()\n",
    "        if query:\n",
    "            df = pd.read_sql_query(sql=query, con=engine)\n",
    "            print(f\"Consulta SQL ejecutada y cargada exitosamente en {time.time() - start_time:.2f} segundos.\")\n",
    "        elif table_name:\n",
    "            full_table_name = f'\"{schema_name}\".\"{table_name}\"'\n",
    "            df = pd.read_sql_query(sql=f\"SELECT * FROM {full_table_name}\", con=engine)\n",
    "            print(f\"Tabla '{database_name}.{schema_name}.{table_name}' leída exitosamente en {time.time() - start_time:.2f} segundos.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer datos de la base de datos: {e}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b1467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "productos = sql_to_df(\n",
    "    database_name = 'OyLCL',\n",
    "    schema_name = 'plan_chile',\n",
    "    table_name = 'productos',\n",
    "    query = 'SELECT * FROM OyLCL.plan_chile.productos',\n",
    "    server_address = \"10.156.16.45\\SQL2019\",\n",
    "    username_sql = username_sql,\n",
    "    password_sql = password_sql\n",
    ")\n",
    "productos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4f0631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaciones adicionales\n",
    "productos['cv_id'] = productos['cv_id'].astype('Int64')\n",
    "productos['cv_prod'] = productos['cv_prod'].astype('Int64')\n",
    "productos['cm_kit'] = productos['cm_kit'].astype('Int64')\n",
    "productos['cm_prod'] = productos['cm_prod'].astype('Int64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8995810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join entre ambos dataframe \n",
    "retail_explotado = pd.merge(exp_retail, productos, left_on='Código', right_on='cv_id', how='left')\n",
    "\n",
    "# Calculando nueva demanda\n",
    "retail_explotado['Demanda'] = retail_explotado['Quantidade_Itens'] * retail_explotado['cantidad']\n",
    "retail_explotado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_explotado_final = retail_explotado.drop_duplicates(subset=['Año', 'Ciclo', 'Subciclo', 'Código', 'cv_prod'], keep='first') # Debe ser a nivel de cv_prod y no de kit\n",
    "retail_explotado_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1579b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_explotado_final[retail_explotado_final['cv_id'] != retail_explotado_final['cv_prod']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c81c661",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZMM0164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011c579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para validar que no existan kits\n",
    "test_retail = pd.merge(ZMM0164, exp_retail, left_on='NºMaterial antiguo', right_on='Código', how='left')\n",
    "test_retail[test_retail['Código'].notna() & test_retail['TpMt'] == 'ZEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4e798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Construir df final de retail que incluya campos adicionales al .xlsx original\n",
    "# Crear la columna booleana para 'Canal'\n",
    "retail_explotado_final['Canal'] = 'RETAIL'\n",
    "\n",
    "# Seleccionar las columnas deseadas y renombrar 'cv_id' a 'Código'\n",
    "# Creamos una lista de las columnas que queremos, usando el nuevo nombre para 'cv_id'\n",
    "df_demanda_retail = retail_explotado_final[[\n",
    "    'Año',\n",
    "    'Ciclo',\n",
    "    'Subciclo',\n",
    "    'cv_id', # Seleccionamos 'cv_id' primero para luego renombrarlo\n",
    "    'cv_prod', # Incorporamos el cv del kit y del producto resultante\n",
    "    'Área',\n",
    "    'Canal', # Incluimos la nueva columna booleana\n",
    "    'Demanda'\n",
    "]].rename(columns={'cv_id': 'Código_Kit', 'cv_prod': 'Código_Producto'}) # Renombramos la columna 'cv_id' a 'Código'\n",
    "\n",
    "# Estandariar campos de año y ciclo\n",
    "df_demanda_retail['Año'] = df_demanda_retail['Año'].str.replace('FY', '').astype(int) + 2000\n",
    "df_demanda_retail['Ciclo'] = df_demanda_retail['Ciclo'].str.replace('C', '').str.lstrip('0').astype(int)\n",
    "\n",
    "df_demanda_retail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f98ca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demanda_retail.drop(columns = ['Área'], inplace = True)\n",
    "\n",
    "df_demanda_retail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e17153",
   "metadata": {},
   "source": [
    "# Procesamiento VD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857c76b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Obtener los datos de demanda de VD\n",
    "\n",
    "path_vd = r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\Carga de demanda VD.xlsx\"\n",
    "\n",
    "exp_vd = crear_dataframe(path_vd)\n",
    "\n",
    "# 2. Aislar el número\n",
    "exp_vd['Cód - Descripción'] = exp_vd['Cód - Descripción'].str.extract('(\\d+)')[0].astype('Int64')\n",
    "\n",
    "# 3. LEFT JOIN con tabla productos\n",
    "# Left join entre ambos dataframe \n",
    "vd_explotado = pd.merge(exp_vd, productos, left_on='Cód - Descripción', right_on='cv_id', how='left')\n",
    "\n",
    "# Calculando nueva demanda\n",
    "vd_explotado['Demanda'] = vd_explotado['Cantidad Itens'] * vd_explotado['cantidad']\n",
    "vd_explotado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9087b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Filtrar duplicados a nivel de cv_prod y nos quedamos con la primera ocurrencia\n",
    "vd_explotado_final = vd_explotado.drop_duplicates(subset=['Año', 'Ciclo', 'Subciclo', 'Cód - Descripción', 'cv_prod'], keep='first')\n",
    "vd_explotado_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0945deda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Procesamiento de estandarización\n",
    "# Construir df final de vd que incluya campos adicionales al .xlsx original\n",
    "# Crear la columna booleana para 'Canal'\n",
    "vd_explotado_final['Canal'] = 'VD'\n",
    "\n",
    "# Seleccionar las columnas deseadas y renombrar 'cv_id' a 'Código'\n",
    "# Creamos una lista de las columnas que queremos, usando el nuevo nombre para 'cv_id'\n",
    "df_demanda_vd = vd_explotado_final[[\n",
    "    'Año',\n",
    "    'Ciclo',\n",
    "    'Subciclo',\n",
    "    'cv_id', # Seleccionamos 'cv_id' primero para luego renombrarlo\n",
    "    'cv_prod', # Incorporamos el cv del kit y del producto resultante\n",
    "    'Canal', # Incluimos la nueva columna booleana\n",
    "    'Demanda'\n",
    "]].rename(columns={'cv_id': 'Código_Kit', 'cv_prod': 'Código_Producto'}) # Renombramos la columnas de cv a sus respectivas descripciones\n",
    "\n",
    "# Estandariar campos de ciclo y subciclo\n",
    "df_demanda_vd['Ciclo'] = df_demanda_vd['Ciclo'].str.replace('Ciclo ', '').str.lstrip('0').astype(int)\n",
    "df_demanda_vd['Subciclo'] = df_demanda_vd['Subciclo'].str.replace('Subciclo ', '')\n",
    "df_demanda_vd['Demanda'] = df_demanda_vd['Demanda'].astype(int)\n",
    "\n",
    "df_demanda_vd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bb0bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesamiento VD Unificado\n",
    "path_vd = r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\Carga de demanda VD.xlsx\"\n",
    "\n",
    "exp_vd = crear_dataframe(path_vd)\n",
    "\n",
    "# 2. Aislar el número\n",
    "exp_vd['Cód - Descripción'] = exp_vd['Cód - Descripción'].str.extract('(\\d+)')[0].astype('Int64')\n",
    "\n",
    "# 3. LEFT JOIN con tabla productos\n",
    "# Left join entre ambos dataframe \n",
    "vd_explotado = pd.merge(exp_vd, productos, left_on='Cód - Descripción', right_on='cv_id', how='left')\n",
    "\n",
    "# Calculando nueva demanda\n",
    "vd_explotado['Demanda'] = vd_explotado['Cantidad Itens'] * vd_explotado['cantidad']\n",
    "\n",
    "# 4. Filtrar duplicados a nivel de cv_prod y nos quedamos con la primera ocurrencia\n",
    "vd_explotado_final = vd_explotado.drop_duplicates(subset=['Año', 'Ciclo', 'Subciclo', 'Cód - Descripción', 'cv_prod'], keep='first')\n",
    "\n",
    "# 5. Procesamiento de estandarización\n",
    "# Construir df final de vd que incluya campos adicionales al .xlsx original\n",
    "# Crear la columna booleana para 'Canal'\n",
    "vd_explotado_final['Canal'] = 'VD'\n",
    "\n",
    "# Seleccionar las columnas deseadas y renombrar 'cv_id' a 'Código'\n",
    "# Creamos una lista de las columnas que queremos, usando el nuevo nombre para 'cv_id'\n",
    "df_demanda_vd = vd_explotado_final[[\n",
    "    'Año',\n",
    "    'Ciclo',\n",
    "    'Subciclo',\n",
    "    'cv_id', # Seleccionamos 'cv_id' primero para luego renombrarlo\n",
    "    'cv_prod', # Incorporamos el cv del kit y del producto resultante\n",
    "    'Canal', # Incluimos la nueva columna booleana\n",
    "    'Demanda'\n",
    "]].rename(columns={'cv_id': 'Código_Kit', 'cv_prod': 'Código_Producto'}) # Renombramos la columnas de cv a sus respectivas descripciones\n",
    "\n",
    "# Estandariar campos de ciclo y subciclo\n",
    "df_demanda_vd['Ciclo'] = df_demanda_vd['Ciclo'].str.replace('Ciclo ', '').str.lstrip('0').astype(int)\n",
    "df_demanda_vd['Subciclo'] = df_demanda_vd['Subciclo'].str.replace('Subciclo ', '')\n",
    "df_demanda_vd['Demanda'] = df_demanda_vd['Demanda'].astype(int)\n",
    "\n",
    "df_demanda_vd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58ff5f7",
   "metadata": {},
   "source": [
    "# Procesamiento VOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abd5ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Obtener los datos de demanda de VOL\n",
    "\n",
    "path_vol = r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\Carga de demanda Vol.xlsx\"\n",
    "\n",
    "exp_vol = crear_dataframe(path_vol)\n",
    "\n",
    "# 2. Aislar el número\n",
    "exp_vol['Código'] = exp_vol['Código'].str.replace('P', '').astype(np.int64)\n",
    "\n",
    "# 3. LEFT JOIN con tabla productos\n",
    "# Left join entre ambos dataframe \n",
    "vol_explotado = pd.merge(exp_vol, productos, left_on='Código', right_on='cv_id', how='left')\n",
    "\n",
    "# Calculando nueva demanda\n",
    "vol_explotado['Demanda'] = vol_explotado['Quantidade_Itens'] * vol_explotado['cantidad']\n",
    "\n",
    "# 4. Filtrar duplicados a nivel de cv_prod y nos quedamos con la primera ocurrencia\n",
    "vol_explotado_final = vol_explotado.drop_duplicates(subset=['Año', 'Ciclo', 'Subciclo', 'Código', 'cv_prod'], keep='first')\n",
    "\n",
    "# 5. Procesamiento de estandarización\n",
    "# Construir df final de vd que incluya campos adicionales al .xlsx original\n",
    "# Crear la columna booleana para 'Canal'\n",
    "vol_explotado_final['Canal'] = 'VOL'\n",
    "\n",
    "# Seleccionar las columnas deseadas y renombrar 'cv_id' a 'Código'\n",
    "# Creamos una lista de las columnas que queremos, usando el nuevo nombre para 'cv_id'\n",
    "df_demanda_vol = vol_explotado_final[[\n",
    "    'Año',\n",
    "    'Ciclo',\n",
    "    'Subciclo',\n",
    "    'cv_id', # Seleccionamos 'cv_id' primero para luego renombrarlo\n",
    "    'cv_prod', # Incorporamos el cv del kit y del producto resultante\n",
    "    'Canal', # Incluimos la nueva columna booleana\n",
    "    'Demanda'\n",
    "]].rename(columns={'cv_id': 'Código_Kit', 'cv_prod': 'Código_Producto'}) # Renombramos la columnas de cv a sus respectivas descripciones\n",
    "\n",
    "# Estandariar campos de ciclo y subciclo\n",
    "# Estandariar campos de año y ciclo\n",
    "df_demanda_vol['Año'] = df_demanda_vol['Año'].str.replace('FY', '').astype(int) + 2000\n",
    "df_demanda_vol['Ciclo'] = df_demanda_vol['Ciclo'].str.replace('C', '').str.lstrip('0').astype(int)\n",
    "\n",
    "df_demanda_vol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb1e3fe",
   "metadata": {},
   "source": [
    "# Procesamieto Unificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf4f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def procesar_demanda_vol(ruta_archivo_vol, df_productos): # Falta validar que sea el mismo código para retail\n",
    "    \"\"\"\n",
    "    Procesa un archivo de demanda de VOL (Venta por Otros Canales) y lo combina\n",
    "    con un DataFrame de productos para calcular la demanda final y estandarizar\n",
    "    los datos.\n",
    "\n",
    "    Args:\n",
    "        ruta_archivo_vol (str): La ruta completa al archivo Excel (.xlsx) de demanda de VOL.\n",
    "        df_productos (pd.DataFrame): Un DataFrame de pandas que contiene la\n",
    "                                      información de los productos, incluyendo\n",
    "                                      la columna 'cv_id' para el join.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un DataFrame procesado con la demanda estandarizada de VOL,\n",
    "                      incluyendo las columnas 'Año', 'Ciclo', 'Subciclo',\n",
    "                      'Código_Kit', 'Código_Producto', 'Canal' y 'Demanda'.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Obtener los datos de demanda de VOL\n",
    "    # Se asume que 'crear_dataframe' es una función existente que carga el Excel.\n",
    "    # Si no, se puede reemplazar por pd.read_excel(ruta_archivo_vol)\n",
    "    try:\n",
    "        exp_vol = pd.read_excel(ruta_archivo_vol)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: El archivo no se encontró en la ruta: {ruta_archivo_vol}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo Excel: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 2. Aislar el número del código de producto\n",
    "    # Se reemplaza 'P' y se convierte a tipo entero de 64 bits para asegurar compatibilidad.\n",
    "    exp_vol['Código'] = exp_vol['Código'].str.replace('P', '', regex=False).astype(np.int64)\n",
    "\n",
    "    # 3. LEFT JOIN con tabla productos\n",
    "    # Combina los DataFrames 'exp_vol' y 'df_productos' usando 'Código' y 'cv_id'.\n",
    "    # Si 'df_productos' no tiene 'cv_id', se generaría un error.\n",
    "    if 'cv_id' not in df_productos.columns:\n",
    "        print(\"Error: El DataFrame de productos debe contener la columna 'cv_id'.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    vol_explotado = pd.merge(exp_vol, df_productos, left_on='Código', right_on='cv_id', how='left')\n",
    "\n",
    "    # Calcula la nueva demanda multiplicando la cantidad de ítems por la cantidad del producto.\n",
    "    vol_explotado['Demanda'] = vol_explotado['Quantidade_Itens'] * vol_explotado['cantidad']\n",
    "\n",
    "    # 4. Filtrar duplicados a nivel de 'cv_prod'\n",
    "    # Elimina filas duplicadas basándose en las columnas especificadas,\n",
    "    # manteniendo solo la primera ocurrencia.\n",
    "    vol_explotado_final = vol_explotado.drop_duplicates(\n",
    "        subset=['Año', 'Ciclo', 'Subciclo', 'Código', 'cv_prod'], keep='first'\n",
    "    )\n",
    "\n",
    "    # 5. Procesamiento de estandarización\n",
    "    # Asigna el valor 'VOL' a la nueva columna 'Canal'.\n",
    "    vol_explotado_final['Canal'] = 'VOL'\n",
    "\n",
    "    # Selecciona y renombra las columnas finales para el DataFrame de salida.\n",
    "    df_demanda_vol = vol_explotado_final[[\n",
    "        'Año',\n",
    "        'Ciclo',\n",
    "        'Subciclo',\n",
    "        'cv_id',\n",
    "        'cv_prod',\n",
    "        'Canal',\n",
    "        'Demanda'\n",
    "    ]].rename(columns={'cv_id': 'Código_Kit', 'cv_prod': 'Código_Producto'})\n",
    "\n",
    "    # Estandariza los campos de 'Año' y 'Ciclo'.\n",
    "    # El año se convierte a un formato de cuatro dígitos (ej., 'FY23' a '2023').\n",
    "    df_demanda_vol['Año'] = df_demanda_vol['Año'].astype(str).str.replace('FY', '', regex=False).astype(int) + 2000\n",
    "    # El ciclo se convierte a un entero sin ceros iniciales (ej., 'C01' a '1').\n",
    "    df_demanda_vol['Ciclo'] = df_demanda_vol['Ciclo'].astype(str).str.replace('C', '', regex=False).str.lstrip('0').astype(int)\n",
    "\n",
    "    return df_demanda_vol\n",
    "\n",
    "def procesar_demanda_retail(ruta_archivo_vol, df_productos): # Falta validar que sea el mismo código para retail\n",
    "    \"\"\"\n",
    "    Procesa un archivo de demanda de RETAIL (Venta por Otros Canales) y lo combina\n",
    "    con un DataFrame de productos para calcular la demanda final y estandarizar\n",
    "    los datos.\n",
    "\n",
    "    Args:\n",
    "        ruta_archivo_vol (str): La ruta completa al archivo Excel (.xlsx) de demanda de RETAIL.\n",
    "        df_productos (pd.DataFrame): Un DataFrame de pandas que contiene la\n",
    "                                      información de los productos, incluyendo\n",
    "                                      la columna 'cv_id' para el join.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un DataFrame procesado con la demanda estandarizada de RETAIL,\n",
    "                      incluyendo las columnas 'Año', 'Ciclo', 'Subciclo',\n",
    "                      'Código_Kit', 'Código_Producto', 'Canal' y 'Demanda'.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Obtener los datos de demanda de RETAIL\n",
    "    # Se asume que 'crear_dataframe' es una función existente que carga el Excel.\n",
    "    # Si no, se puede reemplazar por pd.read_excel(ruta_archivo_vol)\n",
    "    try:\n",
    "        exp_vol = pd.read_excel(ruta_archivo_vol)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: El archivo no se encontró en la ruta: {ruta_archivo_vol}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo Excel: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 2. Aislar el número del código de producto\n",
    "    # Se reemplaza 'P' y se convierte a tipo entero de 64 bits para asegurar compatibilidad.\n",
    "    exp_vol['Código'] = exp_vol['Código'].str.replace('P', '', regex=False).astype(np.int64)\n",
    "\n",
    "    # 3. LEFT JOIN con tabla productos\n",
    "    # Combina los DataFrames 'exp_vol' y 'df_productos' usando 'Código' y 'cv_id'.\n",
    "    # Si 'df_productos' no tiene 'cv_id', se generaría un error.\n",
    "    if 'cv_id' not in df_productos.columns:\n",
    "        print(\"Error: El DataFrame de productos debe contener la columna 'cv_id'.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    vol_explotado = pd.merge(exp_vol, df_productos, left_on='Código', right_on='cv_id', how='left')\n",
    "\n",
    "    # Calcula la nueva demanda multiplicando la cantidad de ítems por la cantidad del producto.\n",
    "    vol_explotado['Demanda'] = vol_explotado['Quantidade_Itens'] * vol_explotado['cantidad']\n",
    "\n",
    "    # 4. Filtrar duplicados a nivel de 'cv_prod'\n",
    "    # Elimina filas duplicadas basándose en las columnas especificadas,\n",
    "    # manteniendo solo la primera ocurrencia.\n",
    "    vol_explotado_final = vol_explotado.drop_duplicates(\n",
    "        subset=['Año', 'Ciclo', 'Subciclo', 'Código', 'cv_prod'], keep='first'\n",
    "    )\n",
    "\n",
    "    # 5. Procesamiento de estandarización\n",
    "    # Asigna el valor 'VOL' a la nueva columna 'Canal'.\n",
    "    vol_explotado_final['Canal'] = 'RETAIL'\n",
    "\n",
    "    # Selecciona y renombra las columnas finales para el DataFrame de salida.\n",
    "    df_demanda_vol = vol_explotado_final[[\n",
    "        'Año',\n",
    "        'Ciclo',\n",
    "        'Subciclo',\n",
    "        'cv_id',\n",
    "        'cv_prod',\n",
    "        'Canal',\n",
    "        'Demanda'\n",
    "    ]].rename(columns={'cv_id': 'Código_Kit', 'cv_prod': 'Código_Producto'})\n",
    "\n",
    "    # Estandariza los campos de 'Año' y 'Ciclo'.\n",
    "    # El año se convierte a un formato de cuatro dígitos (ej., 'FY23' a '2023').\n",
    "    df_demanda_vol['Año'] = df_demanda_vol['Año'].astype(str).str.replace('FY', '', regex=False).astype(int) + 2000\n",
    "    # El ciclo se convierte a un entero sin ceros iniciales (ej., 'C01' a '1').\n",
    "    df_demanda_vol['Ciclo'] = df_demanda_vol['Ciclo'].astype(str).str.replace('C', '', regex=False).str.lstrip('0').astype(int)\n",
    "\n",
    "    return df_demanda_vol\n",
    "\n",
    "def procesar_demanda_vd(ruta_archivo_vd, df_productos):\n",
    "    \"\"\"\n",
    "    Procesa un archivo de demanda de VD (Venta Directa) y lo combina\n",
    "    con un DataFrame de productos para calcular la demanda final y estandarizar\n",
    "    los datos.\n",
    "\n",
    "    Args:\n",
    "        ruta_archivo_vd (str): La ruta completa al archivo Excel (.xlsx) de demanda de VD.\n",
    "        df_productos (pd.DataFrame): Un DataFrame de pandas que contiene la\n",
    "                                      información de los productos, incluyendo\n",
    "                                      la columna 'cv_id' para el join.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un DataFrame procesado con la demanda estandarizada de VD,\n",
    "                      incluyendo las columnas 'Año', 'Ciclo', 'Subciclo',\n",
    "                      'Código_Kit', 'Código_Producto', 'Canal' y 'Demanda'.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Obtener los datos de demanda de VD\n",
    "    # Se asume que 'crear_dataframe' es una función existente que carga el Excel.\n",
    "    # Si no, se puede reemplazar por pd.read_excel(ruta_archivo_vd)\n",
    "    try:\n",
    "        exp_vd = pd.read_excel(ruta_archivo_vd)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: El archivo no se encontró en la ruta: {ruta_archivo_vd}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo Excel: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 2. Aislar el número del código de producto\n",
    "    # Se extraen los dígitos del código y se convierten a Int64 para manejar valores nulos.\n",
    "    exp_vd['Cód - Descripción'] = exp_vd['Cód - Descripción'].astype(str).str.extract(r'(\\d+)')[0].astype('Int64')\n",
    "\n",
    "    # 3. LEFT JOIN con tabla productos\n",
    "    # Se verifica si 'cv_id' existe en el DataFrame de productos antes de intentar el merge.\n",
    "    if 'cv_id' not in df_productos.columns:\n",
    "        print(\"Error: El DataFrame de productos debe contener la columna 'cv_id'.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    vd_explotado = pd.merge(exp_vd, df_productos, left_on='Cód - Descripción', right_on='cv_id', how='left')\n",
    "\n",
    "    # Calculando nueva demanda\n",
    "    vd_explotado['Demanda'] = vd_explotado['Cantidad Itens'] * vd_explotado['cantidad']\n",
    "\n",
    "    # 4. Filtrar duplicados a nivel de 'cv_prod'\n",
    "    # Elimina filas duplicadas basándose en las columnas especificadas,\n",
    "    # manteniendo solo la primera ocurrencia.\n",
    "    vd_explotado_final = vd_explotado.drop_duplicates(\n",
    "        subset=['Año', 'Ciclo', 'Subciclo', 'Cód - Descripción', 'cv_prod'], keep='first'\n",
    "    )\n",
    "\n",
    "    # 5. Procesamiento de estandarización\n",
    "    # Asigna el valor 'VD' a la nueva columna 'Canal'.\n",
    "    vd_explotado_final['Canal'] = 'VD'\n",
    "\n",
    "    # Selecciona y renombra las columnas finales para el DataFrame de salida.\n",
    "    df_demanda_vd = vd_explotado_final[[\n",
    "        'Año',\n",
    "        'Ciclo',\n",
    "        'Subciclo',\n",
    "        'cv_id',\n",
    "        'cv_prod',\n",
    "        'Canal',\n",
    "        'Demanda'\n",
    "    ]].rename(columns={'cv_id': 'Código_Kit', 'cv_prod': 'Código_Producto'})\n",
    "\n",
    "    # Estandariza los campos de 'Ciclo' y 'Subciclo'.\n",
    "    # El ciclo se convierte a un entero sin el prefijo 'Ciclo '.\n",
    "    df_demanda_vd['Ciclo'] = df_demanda_vd['Ciclo'].astype(str).str.replace('Ciclo ', '', regex=False).str.lstrip('0').astype(int)\n",
    "    # El subciclo se limpia del prefijo 'Subciclo '.\n",
    "    df_demanda_vd['Subciclo'] = df_demanda_vd['Subciclo'].astype(str).str.replace('Subciclo ', '', regex=False)\n",
    "    # Asegura que la demanda sea de tipo entero.\n",
    "    df_demanda_vd['Demanda'] = df_demanda_vd['Demanda'].astype('Int64')\n",
    "\n",
    "    return df_demanda_vd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c9d7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demanda de retail\n",
    "df_demanda_retail = procesar_demanda_retail(r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\Demanda Retail.xlsx\", productos)\n",
    "df_demanda_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6510673",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demanda_retail[df_demanda_retail['Ciclo'] == 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a675d3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demanda de vol\n",
    "df_demanda_vol = procesar_demanda_vol(r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\Carga de demanda Vol.xlsx\", productos)\n",
    "df_demanda_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a738e9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demanda de vd\n",
    "df_demanda_vd = procesar_demanda_vd(r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\Carga de demanda VD.xlsx\", productos)\n",
    "df_demanda_vd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9191a683",
   "metadata": {},
   "source": [
    "# Procesar distribución curva demanda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ac90c5",
   "metadata": {},
   "source": [
    "## Demanda Retail "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e3f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTE MÉTODO FUNCIONA BIEN PARA RETAIL PERO CON DEBUGGER, AÚN HAY ERRORES DE DECIMALES\n",
    "\n",
    "def distribuir_demanda(df_demanda_retail, ruta_curva_demanda):\n",
    "    \"\"\"\n",
    "    Distribuye la demanda del dataframe df_demanda_retail en los días del ciclo\n",
    "    según los porcentajes definidos en la curva de demanda.\n",
    "    Maneja valores de porcentaje con coma decimal y símbolo '%'.\n",
    "    Estandariza los campos 'Año' y 'Ciclo' en df_curva para que coincidan con df_demanda_retail.\n",
    "\n",
    "    Args:\n",
    "        df_demanda_retail (pd.DataFrame): DataFrame con los campos Año, Ciclo, Subciclo,\n",
    "                                        Código_Kit, Código_Producto, Canal y Demanda.\n",
    "        ruta_curva_demanda (str): Ruta al archivo Excel (.xlsx) o CSV que contiene la curva de demanda.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con la demanda distribuida por día.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Cargar la curva de demanda.\n",
    "        if ruta_curva_demanda.endswith('.xlsx'):\n",
    "            df_curva = pd.read_excel(ruta_curva_demanda)\n",
    "            print(f\"DEBUG: df_curva cargado desde XLSX. Primeras filas ANTES de limpiar y estandarizar:\\n{df_curva.head()}\")\n",
    "        elif ruta_curva_demanda.endswith('.csv'):\n",
    "            df_curva = pd.read_csv(ruta_curva_demanda, decimal=',')\n",
    "            print(f\"DEBUG: df_curva cargado desde CSV. Primeras filas ANTES de limpiar y estandarizar:\\n{df_curva.head()}\")\n",
    "        else:\n",
    "            raise ValueError(\"Formato de archivo no soportado. Por favor, use .xlsx o .csv\")\n",
    "\n",
    "        # Identificar las columnas que representan los días en la curva de demanda\n",
    "        columnas_dias_curva = [col for col in df_curva.columns if 'Día' in col]\n",
    "\n",
    "        if not columnas_dias_curva:\n",
    "            raise ValueError(\"No se encontraron columnas de días en la curva de demanda. \"\n",
    "                             \"Asegúrese de que los nombres de las columnas contengan 'Día'.\")\n",
    "\n",
    "        # --- INICIO: Estandarización de 'Año' y 'Ciclo' en df_curva ---\n",
    "        print(\"\\nDEBUG: Estandarizando columnas 'Año' y 'Ciclo' en df_curva...\")\n",
    "        if 'Año' in df_curva.columns:\n",
    "            # Asumimos que 'Año' en df_curva puede ser 'FY23' o similar.\n",
    "            # Convertir a string, quitar 'FY', convertir a int, y sumar 2000.\n",
    "            # Usamos errors='coerce' para convertir a NaN si hay algún valor que no se ajusta\n",
    "            # y luego rellenamos NaN con un valor por defecto o se gestiona.\n",
    "            # Para este caso, si no se puede convertir, podría ser un año completo ya.\n",
    "            df_curva['Año_str_temp'] = df_curva['Año'].astype(str).str.replace('FY', '', regex=False)\n",
    "            df_curva['Año_temp'] = pd.to_numeric(df_curva['Año_str_temp'], errors='coerce')\n",
    "            # Si el año ya es 4 dígitos (ej. 2023), no se le suma 2000.\n",
    "            df_curva['Año'] = df_curva.apply(\n",
    "                lambda row: int(row['Año_temp']) + 2000 if pd.notna(row['Año_temp']) and len(str(int(row['Año_temp']))) <= 2 else int(row['Año']),\n",
    "                axis=1\n",
    "            )\n",
    "            df_curva.drop(columns=['Año_str_temp', 'Año_temp'], inplace=True, errors='ignore')\n",
    "            df_curva['Año'] = df_curva['Año'].astype(int) # Asegurar que sea int\n",
    "\n",
    "            # Pequeña verificación si el año en df_demanda_retail es solo de 2 dígitos,\n",
    "            # lo que no es el caso en tu ejemplo, pero para robustez.\n",
    "            # df_demanda_retail['Año'] = df_demanda_retail['Año'].astype(str).apply(lambda x: int(x) + 2000 if len(x) == 2 else int(x))\n",
    "\n",
    "\n",
    "        if 'Ciclo' in df_curva.columns:\n",
    "            # Asumimos que 'Ciclo' en df_curva puede ser 'C01' o similar.\n",
    "            # Convertir a string, quitar 'C', eliminar ceros iniciales, convertir a int.\n",
    "            df_curva['Ciclo'] = df_curva['Ciclo'].astype(str).str.replace('C', '', regex=False).str.lstrip('0')\n",
    "            df_curva['Ciclo'] = pd.to_numeric(df_curva['Ciclo'], errors='coerce').fillna(0).astype(int) # Coerce y rellenar NaN por si acaso\n",
    "\n",
    "        print(f\"DEBUG: df_curva después de estandarizar 'Año' y 'Ciclo' (primeras filas):\\n{df_curva.head()}\")\n",
    "        print(f\"DEBUG: Tipos de datos de 'Año' y 'Ciclo' en df_curva: Año={df_curva['Año'].dtype}, Ciclo={df_curva['Ciclo'].dtype}\")\n",
    "        # --- FIN: Estandarización de 'Año' y 'Ciclo' en df_curva ---\n",
    "\n",
    "\n",
    "        # --- APLICAR LA LIMPIEZA Y CONVERSIÓN DE PORCENTAJES (esto ya estaba y se mantiene) ---\n",
    "        print(f\"\\nDEBUG: Procesando columnas de días: {columnas_dias_curva}\")\n",
    "        for col in columnas_dias_curva:\n",
    "            original_dtype = df_curva[col].dtype\n",
    "            df_curva[col] = (\n",
    "                df_curva[col]\n",
    "                .astype(str)                                # Asegurarse de que sea string\n",
    "                .str.replace('%', '', regex=False)          # Eliminar el símbolo '%'\n",
    "                .str.replace(',', '.', regex=False)          # Reemplazar la coma por el punto decimal\n",
    "            )\n",
    "            df_curva[col] = pd.to_numeric(df_curva[col], errors='coerce') # Convertir a numérico, NaNs para errores\n",
    "            print(f\"DEBUG: Columna '{col}' - Tipo original: {original_dtype}, Tipo después de limpieza: {df_curva[col].dtype}\")\n",
    "\n",
    "        print(f\"\\nDEBUG: df_curva después de limpiar porcentajes (primeras filas):\\n{df_curva.head()}\")\n",
    "        print(f\"DEBUG: Tipos de datos de df_curva después de limpieza:\\n{df_curva[columnas_dias_curva].dtypes}\")\n",
    "\n",
    "\n",
    "        # Preparar el dataframe de resultados\n",
    "        df_resultado = []\n",
    "\n",
    "        # Iterar sobre cada fila de df_demanda_retail para distribuir la demanda\n",
    "        for index, row in df_demanda_retail.iterrows():\n",
    "            # Asegurarse de que 'Año', 'Ciclo', 'Subciclo' de df_demanda_retail sean int para la comparación\n",
    "            año = row['Año']\n",
    "            ciclo = row['Ciclo']\n",
    "            subciclo = row['Subciclo']\n",
    "            demanda_total = row['Demanda']\n",
    "\n",
    "            print(f\"\\nDEBUG: Procesando fila de demanda - Año: {año}, Ciclo: {ciclo}, Subciclo: {subciclo}, Demanda Total: {demanda_total}\")\n",
    "\n",
    "            # Buscar la curva de demanda correspondiente a Año, Ciclo y Subciclo\n",
    "            curva_especifica = df_curva[\n",
    "                (df_curva['Año'] == año) &\n",
    "                (df_curva['Ciclo'] == ciclo) &\n",
    "                (df_curva['Subciclo'] == subciclo)\n",
    "            ]\n",
    "\n",
    "            if not curva_especifica.empty:\n",
    "                curva_especifica = curva_especifica.iloc[0] # Tomar la primera coincidencia\n",
    "                print(f\"DEBUG: Curva específica ENCONTRADA para {año}-{ciclo}-{subciclo}. Valores de porcentaje:\\n{curva_especifica[columnas_dias_curva].to_string()}\")\n",
    "\n",
    "                max_dia_num = 0\n",
    "                for col_dia in columnas_dias_curva:\n",
    "                    try:\n",
    "                        max_dia_num = max(max_dia_num, int(col_dia.replace('Día ', '')))\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                demanda_distribuida_por_dia = {f'Demanda_Dia_{i+1}' : 0 for i in range(max_dia_num)}\n",
    "\n",
    "\n",
    "                for col_dia in columnas_dias_curva:\n",
    "                    try:\n",
    "                        dia_num = int(col_dia.replace('Día ', ''))\n",
    "                        valor_porcentaje_curva = curva_especifica[col_dia]\n",
    "                        porcentaje_dia = valor_porcentaje_curva if pd.notna(valor_porcentaje_curva) else 0 \n",
    "                        demanda_distribuida = demanda_total * porcentaje_dia\n",
    "\n",
    "                        print(f\"DEBUG: Día: {col_dia}, Valor Curva: {valor_porcentaje_curva}, Porcentaje (decimal): {porcentaje_dia}, Demanda Distribuida: {demanda_distribuida}\")\n",
    "\n",
    "                        demanda_distribuida_por_dia[f'Demanda_Dia_{dia_num}'] = demanda_distribuida\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                    except KeyError:\n",
    "                        print(f\"ADVERTENCIA: La columna '{col_dia}' no se encontró en la curva específica. Se asignará 0.\")\n",
    "                        # Asegurar que se asigna 0 si el día no existe en la curva (aunque el patrón de columnas_dias_curva debería evitar esto)\n",
    "                        if f'Demanda_Dia_{dia_num}' not in demanda_distribuida_por_dia:\n",
    "                             demanda_distribuida_por_dia[f'Demanda_Dia_{dia_num}'] = 0\n",
    "\n",
    "\n",
    "                nueva_fila = row.to_dict()\n",
    "                nueva_fila.update(demanda_distribuida_por_dia)\n",
    "                df_resultado.append(nueva_fila)\n",
    "            else:\n",
    "                print(f\"DEBUG: NO se encontró curva específica para Año: {año}, Ciclo: {ciclo}, Subciclo: {subciclo}. Asignando 0 a todos los días.\")\n",
    "                nueva_fila = row.to_dict()\n",
    "                max_dia_num = 0\n",
    "                for col_dia in columnas_dias_curva:\n",
    "                    try:\n",
    "                        max_dia_num = max(max_dia_num, int(col_dia.replace('Día ', '')))\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                for i in range(max_dia_num):\n",
    "                     nueva_fila[f'Demanda_Dia_{i+1}'] = 0\n",
    "                df_resultado.append(nueva_fila)\n",
    "\n",
    "        return pd.DataFrame(df_resultado)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: El archivo no fue encontrado en la ruta '{ruta_curva_demanda}'.\")\n",
    "        return pd.DataFrame()\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Falta una columna esperada en uno de los DataFrames. Detalle: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error inesperado: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3241e6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versión 2 final\n",
    "import pandas as pd\n",
    "\n",
    "def distribuir_demanda(df_demanda_retail, ruta_curva_demanda): # Sirve para vol y retail\n",
    "    \"\"\"\n",
    "    Distribuye la demanda del dataframe df_demanda_retail en los días del ciclo\n",
    "    según los porcentajes definidos en la curva de demanda.\n",
    "    Maneja valores de porcentaje con coma decimal y símbolo '%'.\n",
    "    Estandariza los campos 'Año' y 'Ciclo' en df_curva para que coincidan con df_demanda_retail.\n",
    "    Mantiene los nombres originales de las columnas de días (ej. 'Día 1', 'Día 2').\n",
    "\n",
    "    Args:\n",
    "        df_demanda_retail (pd.DataFrame): DataFrame con los campos Año, Ciclo, Subciclo,\n",
    "                                        Código_Kit, Código_Producto, Canal y Demanda.\n",
    "        ruta_curva_demanda (str): Ruta al archivo Excel (.xlsx) o CSV que contiene la curva de demanda.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con los datos de df_demanda_retail y las columnas 'Día N'\n",
    "                      que contienen la demanda distribuida.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Cargar la curva de demanda.\n",
    "        if ruta_curva_demanda.endswith('.xlsx'):\n",
    "            df_curva = pd.read_excel(ruta_curva_demanda)\n",
    "        elif ruta_curva_demanda.endswith('.csv'):\n",
    "            df_curva = pd.read_csv(ruta_curva_demanda, decimal=',')\n",
    "        else:\n",
    "            raise ValueError(\"Formato de archivo no soportado. Por favor, use .xlsx o .csv\")\n",
    "\n",
    "        # Identificar las columnas que representan los días en la curva de demanda\n",
    "        columnas_dias_curva = [col for col in df_curva.columns if 'Día' in col]\n",
    "\n",
    "        if not columnas_dias_curva:\n",
    "            raise ValueError(\"No se encontraron columnas de días en la curva de demanda. \"\n",
    "                             \"Asegúrese de que los nombres de las columnas contengan 'Día'.\")\n",
    "\n",
    "        # --- Estandarización de 'Año' y 'Ciclo' en df_curva ---\n",
    "        if 'Año' in df_curva.columns:\n",
    "            df_curva['Año_str_temp'] = df_curva['Año'].astype(str).str.replace('FY', '', regex=False)\n",
    "            df_curva['Año_temp'] = pd.to_numeric(df_curva['Año_str_temp'], errors='coerce')\n",
    "            df_curva['Año'] = df_curva.apply(\n",
    "                lambda row: int(row['Año_temp']) + 2000 if pd.notna(row['Año_temp']) and len(str(int(row['Año_temp']))) <= 2 else int(row['Año']),\n",
    "                axis=1\n",
    "            )\n",
    "            df_curva.drop(columns=['Año_str_temp', 'Año_temp'], inplace=True, errors='ignore')\n",
    "            df_curva['Año'] = df_curva['Año'].astype(int)\n",
    "\n",
    "        if 'Ciclo' in df_curva.columns:\n",
    "            df_curva['Ciclo'] = df_curva['Ciclo'].astype(str).str.replace('C', '', regex=False).str.lstrip('0')\n",
    "            df_curva['Ciclo'] = pd.to_numeric(df_curva['Ciclo'], errors='coerce').fillna(0).astype(int)\n",
    "        # --- FIN Estandarización ---\n",
    "\n",
    "        # --- APLICAR LA LIMPIEZA Y CONVERSIÓN DE PORCENTAJES ---\n",
    "        for col in columnas_dias_curva:\n",
    "            df_curva[col] = (\n",
    "                df_curva[col]\n",
    "                .astype(str)\n",
    "                .str.replace('%', '', regex=False)\n",
    "                .str.replace(',', '.', regex=False)\n",
    "            )\n",
    "            df_curva[col] = pd.to_numeric(df_curva[col], errors='coerce')\n",
    "        # --- FIN Limpieza de Porcentajes ---\n",
    "\n",
    "        # --- Preparar el DataFrame de resultado ---\n",
    "        # Obtener el conjunto único de todas las columnas 'Día N' que existen en df_curva\n",
    "        all_day_columns = sorted([col for col in df_curva.columns if 'Día' in col and col.replace('Día ', '').isdigit()],\n",
    "                                 key=lambda x: int(x.replace('Día ', '')))\n",
    "        \n",
    "        # Crear un df temporal con las columnas de demanda retail + todas las columnas de días inicializadas en 0\n",
    "        df_resultado_base = df_demanda_retail.copy()\n",
    "        for col_dia in all_day_columns:\n",
    "            df_resultado_base[col_dia] = 0.0 # Inicializar con float\n",
    "\n",
    "\n",
    "        # Iterar sobre cada fila de df_demanda_retail (que ahora es df_resultado_base)\n",
    "        for index, row in df_resultado_base.iterrows():\n",
    "            # Asegurarse de que 'Año', 'Ciclo', 'Subciclo' de df_demanda_retail sean int para la comparación\n",
    "            año = row['Año']\n",
    "            ciclo = row['Ciclo']\n",
    "            subciclo = row['Subciclo']\n",
    "            demanda_total = row['Demanda']\n",
    "\n",
    "            # Buscar la curva de demanda correspondiente a Año, Ciclo y Subciclo\n",
    "            curva_especifica = df_curva[\n",
    "                (df_curva['Año'] == año) &\n",
    "                (df_curva['Ciclo'] == ciclo) &\n",
    "                (df_curva['Subciclo'] == subciclo)\n",
    "            ]\n",
    "\n",
    "            if not curva_especifica.empty:\n",
    "                curva_especifica = curva_especifica.iloc[0] # Tomar la primera coincidencia\n",
    "                \n",
    "                # Para cada columna de día identificada en la curva de demanda\n",
    "                for col_dia_name in columnas_dias_curva:\n",
    "                    try:\n",
    "                        valor_porcentaje_curva = curva_especifica[col_dia_name]\n",
    "                        porcentaje_dia = valor_porcentaje_curva if pd.notna(valor_porcentaje_curva) else 0\n",
    "                        demanda_distribuida = demanda_total * porcentaje_dia\n",
    "\n",
    "                        # Asignar la demanda distribuida directamente a la columna 'Día N'\n",
    "                        if col_dia_name in df_resultado_base.columns:\n",
    "                            df_resultado_base.at[index, col_dia_name] = demanda_distribuida\n",
    "                        # else: # No es necesario un else aquí, ya que se inicializan todas las columnas\n",
    "                        #     pass # Si la columna no existe, no se hace nada y se queda con el 0 inicial\n",
    "\n",
    "                    except KeyError:\n",
    "                        # Esto ocurriría si col_dia_name existe en columnas_dias_curva pero no en curva_especifica.\n",
    "                        # Dado cómo se construye columnas_dias_curva, es poco probable para un 'Día N' válido.\n",
    "                        # Se mantendría el 0 inicializado en df_resultado_base.\n",
    "                        pass\n",
    "                    except Exception:\n",
    "                        # Cualquier otro error durante el cálculo de un día, mantiene el 0 inicializado\n",
    "                        pass\n",
    "\n",
    "            # else: # No es necesario un else aquí, las columnas de días ya están en 0 en df_resultado_base\n",
    "                # Si no se encuentra curva, las columnas de días ya están inicializadas a 0\n",
    "                # pass\n",
    "\n",
    "        return df_resultado_base # Retornar el DataFrame base modificado\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: El archivo no fue encontrado en la ruta '{ruta_curva_demanda}'.\")\n",
    "        return pd.DataFrame()\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Falta una columna esperada en uno de los DataFrames. Detalle: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error inesperado: {e}\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e118fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df = distribuir_demanda(df_demanda_retail, r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\Curva Synplin Retail.xlsx\")\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414c7209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asumiendo que ya tienes el DataFrame llamado df\n",
    "# Creamos una lista con los nombres de las columnas desde Demanda_Dia_1 hasta Demanda_Dia_50\n",
    "columnas_a_sumar = [f'Día {i}' for i in range(1, 51)]\n",
    "\n",
    "# Sumamos esas columnas fila por fila y guardamos el resultado en una nueva columna llamada 'SUMA'\n",
    "df['SUMA'] = (df[columnas_a_sumar].sum(axis=1)).round(0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cc9ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\test_curva_retail.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3340b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el Excel normalmente\n",
    "pd.set_option('display.max_columns', None)\n",
    "syn = pd.read_excel(r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\Curva Synplin Retail.xlsx\")\n",
    "\n",
    "# Reemplazar '%' y ',' por '.' y convertir a float\n",
    "columnas_porcentuales = [col for col in syn.columns if 'Día' in col]  # o ajusta según tus nombres\n",
    "\n",
    "for col in columnas_porcentuales:\n",
    "    syn[col] = (\n",
    "        syn[col]\n",
    "        .astype(str)\n",
    "        .str.replace('%', '', regex=False)\n",
    "        .str.replace(',', '.', regex=False)\n",
    "        .astype(float)\n",
    "    )\n",
    "syn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f894ed1",
   "metadata": {},
   "source": [
    "## Obtención de demandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e725e766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtención de cada demanda\n",
    "\n",
    "demanda_retail = distribuir_demanda(df_demanda_retail, r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\Curva Synplin Retail.xlsx\")\n",
    "demanda_retail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1697835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "demanda_vd = distribuir_demanda(df_demanda_vd, r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\Curva Synplin VD.xlsx\")\n",
    "demanda_vd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b88bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "demanda_vol = distribuir_demanda(df_demanda_vol, r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\Curva Synplin Vol.xlsx\")\n",
    "demanda_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10eb3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar los 3 dataframe resultantes\n",
    "\n",
    "\n",
    "# Supongamos que tus tres DataFrames se llaman df1, df2 y df3\n",
    "df_combinado = pd.concat([demanda_vol, demanda_retail, demanda_vd], ignore_index=True)\n",
    "df_combinado.to_excel(r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\Demanda_Explotada.xlsx\", index = False)\n",
    "df_combinado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b619a26f",
   "metadata": {},
   "source": [
    "# Metodo distribución demanda por día del ciclo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc3e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def distribuir_demanda(df_demanda_retail, ruta_curva_demanda, ruta_calendario_ciclos):\n",
    "    \"\"\"\n",
    "    Distribuye la demanda del dataframe df_demanda_retail en los días del ciclo\n",
    "    según los porcentajes definidos en la curva de demanda y ajusta la distribución\n",
    "    basándose en la fecha actual y el calendario de ciclos.\n",
    "\n",
    "    Args:\n",
    "        df_demanda_retail (pd.DataFrame): DataFrame con los campos Año, Ciclo, Subciclo,\n",
    "                                          Código_Kit, Código_Producto, Canal y Demanda.\n",
    "        ruta_curva_demanda (str): Ruta al archivo Excel (.xlsx) o CSV que contiene la curva de demanda.\n",
    "        ruta_calendario_ciclos (str): Ruta al archivo Excel (.xlsx) o CSV que contiene el calendario de ciclos.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con los datos de df_demanda_retail y las columnas\n",
    "                      'Día Ciclo' y 'Demanda Día Ciclo' que contienen la demanda distribuida.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Cargar la curva de demanda.\n",
    "        if ruta_curva_demanda.endswith('.xlsx'):\n",
    "            df_curva = pd.read_excel(ruta_curva_demanda)\n",
    "        elif ruta_curva_demanda.endswith('.csv'):\n",
    "            df_curva = pd.read_csv(ruta_curva_demanda, decimal=',')\n",
    "        else:\n",
    "            raise ValueError(\"Formato de archivo de curva de demanda no soportado. Por favor, use .xlsx o .csv\")\n",
    "\n",
    "        # Cargar el calendario de ciclos.\n",
    "        if ruta_calendario_ciclos.endswith('.xlsx'):\n",
    "            df_calendario = pd.read_excel(ruta_calendario_ciclos)\n",
    "        elif ruta_calendario_ciclos.endswith('.csv'):\n",
    "            df_calendario = pd.read_csv(ruta_calendario_ciclos, decimal=',')\n",
    "        else:\n",
    "            raise ValueError(\"Formato de archivo de calendario no soportado. Por favor, use .xlsx o .csv\")\n",
    "\n",
    "        # Convertir columnas de fecha en df_calendario a datetime\n",
    "        df_calendario['Fecha Inicio'] = pd.to_datetime(df_calendario['Fecha Inicio'])\n",
    "        df_calendario['Fecha Fin'] = pd.to_datetime(df_calendario['Fecha Fin'])\n",
    "\n",
    "        # Identificar las columnas que representan los días en la curva de demanda\n",
    "        columnas_dias_curva = [col for col in df_curva.columns if 'Día' in col]\n",
    "\n",
    "        if not columnas_dias_curva:\n",
    "            raise ValueError(\"No se encontraron columnas de días en la curva de demanda. \"\n",
    "                             \"Asegúrese de que los nombres de las columnas contengan 'Día'.\")\n",
    "\n",
    "        # --- Estandarización de 'Año' y 'Ciclo' en df_curva ---\n",
    "        if 'Año' in df_curva.columns:\n",
    "            df_curva['Año_str_temp'] = df_curva['Año'].astype(str).str.replace('FY', '', regex=False)\n",
    "            df_curva['Año_temp'] = pd.to_numeric(df_curva['Año_str_temp'], errors='coerce')\n",
    "            df_curva['Año'] = df_curva.apply(\n",
    "                lambda row: int(row['Año_temp']) + 2000 if pd.notna(row['Año_temp']) and len(str(int(row['Año_temp']))) <= 2 else int(row['Año']),\n",
    "                axis=1\n",
    "            )\n",
    "            df_curva.drop(columns=['Año_str_temp', 'Año_temp'], inplace=True, errors='ignore')\n",
    "            df_curva['Año'] = df_curva['Año'].astype(int)\n",
    "\n",
    "        if 'Ciclo' in df_curva.columns:\n",
    "            df_curva['Ciclo'] = df_curva['Ciclo'].astype(str).str.replace('C', '', regex=False).str.lstrip('0')\n",
    "            df_curva['Ciclo'] = pd.to_numeric(df_curva['Ciclo'], errors='coerce').fillna(0).astype(int)\n",
    "        # --- FIN Estandarización ---\n",
    "\n",
    "        # --- Estandarización de 'Año' y 'Ciclo' en df_calendario (similar a df_curva si es necesario) ---\n",
    "        if 'Año' in df_calendario.columns:\n",
    "            df_calendario['Año_str_temp'] = df_calendario['Año'].astype(str).str.replace('FY', '', regex=False)\n",
    "            df_calendario['Año_temp'] = pd.to_numeric(df_calendario['Año_str_temp'], errors='coerce')\n",
    "            df_calendario['Año'] = df_calendario.apply(\n",
    "                lambda row: int(row['Año_temp']) + 2000 if pd.notna(row['Año_temp']) and len(str(int(row['Año_temp']))) <= 2 else int(row['Año']),\n",
    "                axis=1\n",
    "            )\n",
    "            df_calendario.drop(columns=['Año_str_temp', 'Año_temp'], inplace=True, errors='ignore')\n",
    "            df_calendario['Año'] = df_calendario['Año'].astype(int)\n",
    "\n",
    "        if 'Ciclo' in df_calendario.columns:\n",
    "            df_calendario['Ciclo'] = df_calendario['Ciclo'].astype(str).str.replace('C', '', regex=False).str.lstrip('0')\n",
    "            df_calendario['Ciclo'] = pd.to_numeric(df_calendario['Ciclo'], errors='coerce').fillna(0).astype(int)\n",
    "        # --- FIN Estandarización de calendario ---\n",
    "\n",
    "\n",
    "        # --- APLICAR LA LIMPIEZA Y CONVERSIÓN DE PORCENTAJES EN df_curva ---\n",
    "        for col in columnas_dias_curva:\n",
    "            df_curva[col] = (\n",
    "                df_curva[col]\n",
    "                .astype(str)\n",
    "                .str.replace('%', '', regex=False)\n",
    "                .str.replace(',', '.', regex=False)\n",
    "            )\n",
    "            df_curva[col] = pd.to_numeric(df_curva[col], errors='coerce')\n",
    "        # --- FIN Limpieza de Porcentajes ---\n",
    "\n",
    "        # DataFrame final para acumular los resultados\n",
    "        df_resultado_final = pd.DataFrame(columns=[\n",
    "            'Año', 'Ciclo', 'Subciclo', 'Código_Kit', 'Código_Producto', 'Canal',\n",
    "            'Demanda', 'Día Ciclo', 'Demanda Día Ciclo'\n",
    "        ])\n",
    "\n",
    "        fecha_hoy = datetime.now() # Ojo: Usa datetime.now() o una fecha fija para pruebas\n",
    "\n",
    "        # Iterar sobre cada fila de df_demanda_retail\n",
    "        for index, row in df_demanda_retail.iterrows():\n",
    "            año = row['Año']\n",
    "            ciclo = row['Ciclo']\n",
    "            subciclo = row['Subciclo']\n",
    "            demanda_total = row['Demanda']\n",
    "\n",
    "            # Buscar la curva de demanda correspondiente\n",
    "            curva_especifica = df_curva[\n",
    "                (df_curva['Año'] == año) &\n",
    "                (df_curva['Ciclo'] == ciclo) &\n",
    "                (df_curva['Subciclo'] == subciclo)\n",
    "            ]\n",
    "\n",
    "            # Buscar el calendario específico\n",
    "            calendario_especifico = df_calendario[\n",
    "                (df_calendario['Año'] == año) &\n",
    "                (df_calendario['Ciclo'] == ciclo) &\n",
    "                (df_calendario['Subciclo'] == subciclo)\n",
    "            ]\n",
    "\n",
    "            if not curva_especifica.empty and not calendario_especifico.empty:\n",
    "                curva_especifica = curva_especifica.iloc[0]\n",
    "                calendario_especifico = calendario_especifico.iloc[0]\n",
    "\n",
    "                fecha_inicio_ciclo = calendario_especifico['Fecha Inicio']\n",
    "                fecha_fin_ciclo = calendario_especifico['Fecha Fin']\n",
    "                dias_curva = calendario_especifico['Dias curva'] # Asumiendo que 'Dias curva' está en el calendario\n",
    "\n",
    "                demanda_distribuida_por_dia = {}\n",
    "                \n",
    "                # Caso 1: fecha_hoy > fecha_fin_ciclo (Ciclo terminado)\n",
    "                if fecha_hoy > fecha_fin_ciclo:\n",
    "                    for i in range(1, int(dias_curva) + 1):\n",
    "                        col_dia_name = f'Día {i}'\n",
    "                        porcentaje_dia = curva_especifica.get(col_dia_name, 0) # Usar .get para manejar posibles faltantes\n",
    "                        demanda_distribuida_por_dia[i] = demanda_total * (porcentaje_dia / 100) # Asumir que porcentajes están en %\n",
    "                \n",
    "                # Caso 2: fecha_hoy < fecha_inicio_ciclo (Ciclo futuro)\n",
    "                elif fecha_hoy < fecha_inicio_ciclo:\n",
    "                    for i in range(1, int(dias_curva) + 1):\n",
    "                        col_dia_name = f'Día {i}'\n",
    "                        porcentaje_dia = curva_especifica.get(col_dia_name, 0)\n",
    "                        demanda_distribuida_por_dia[i] = demanda_total * (porcentaje_dia / 100)\n",
    "                \n",
    "                # Caso 3: fecha_inicio_ciclo <= fecha_hoy <= fecha_fin_ciclo (Ciclo en curso)\n",
    "                else:\n",
    "                    dias_transcurridos = (fecha_hoy - fecha_inicio_ciclo).days + 1\n",
    "                    \n",
    "                    # Calcular la demanda ya \"consumida\" hasta hoy\n",
    "                    porcentaje_acumulado_pasado = 0\n",
    "                    for i in range(1, dias_transcurridos): # Sumar porcentajes de días anteriores a hoy\n",
    "                        col_dia_name = f'Día {i}'\n",
    "                        porcentaje_dia = curva_especifica.get(col_dia_name, 0)\n",
    "                        porcentaje_acumulado_pasado += porcentaje_dia\n",
    "\n",
    "                    # La demanda pendiente es el total\n",
    "                    demanda_pendiente = demanda_total\n",
    "\n",
    "                    # Calcular la suma de los porcentajes de los días restantes\n",
    "                    suma_porcentajes_pendientes = 0\n",
    "                    for i in range(dias_transcurridos, int(dias_curva) + 1):\n",
    "                        col_dia_name = f'Día {i}'\n",
    "                        porcentaje_dia = curva_especifica.get(col_dia_name, 0)\n",
    "                        suma_porcentajes_pendientes += porcentaje_dia\n",
    "\n",
    "                    if suma_porcentajes_pendientes > 0:\n",
    "                        # Distribuir la demanda pendiente en los días restantes manteniendo la proporción\n",
    "                        for i in range(1, int(dias_curva) + 1):\n",
    "                            col_dia_name = f'Día {i}'\n",
    "                            if i < dias_transcurridos:\n",
    "                                # Días pasados tienen 0 demanda, se asume que ya se distribuyó\n",
    "                                demanda_distribuida_por_dia[i] = 0\n",
    "                            else:\n",
    "                                porcentaje_dia_original = curva_especifica.get(col_dia_name, 0)\n",
    "                                # Redistribuir la demanda pendiente\n",
    "                                demanda_distribuida_por_dia[i] = demanda_pendiente * (porcentaje_dia_original / suma_porcentajes_pendientes)\n",
    "                    else:\n",
    "                        # Si no hay días pendientes con porcentaje, la demanda pendiente se vuelve 0 para esos días\n",
    "                        for i in range(1, int(dias_curva) + 1):\n",
    "                            demanda_distribuida_por_dia[i] = 0\n",
    "            else:\n",
    "                # Si no se encuentra curva o calendario, la demanda por día es 0\n",
    "                for i in range(1, int(curva_especifica.get('Dias curva', 0)) + 1 if not curva_especifica.empty else 1):\n",
    "                    demanda_distribuida_por_dia[i] = 0 # Inicializar a 0 si no hay curva/calendario\n",
    "                \n",
    "            # Agregar las filas al DataFrame final\n",
    "            for dia_ciclo, demanda_dia_ciclo in demanda_distribuida_por_dia.items():\n",
    "                new_row = row.to_dict()\n",
    "                new_row['Día Ciclo'] = dia_ciclo\n",
    "                new_row['Demanda Día Ciclo'] = demanda_dia_ciclo\n",
    "                df_resultado_final = pd.concat([df_resultado_final, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "        return df_resultado_final.round(2) # Redondear a 2 decimales para claridad\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Uno de los archivos no fue encontrado.\")\n",
    "        return pd.DataFrame()\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Falta una columna esperada en uno de los DataFrames. Detalle: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    except ValueError as e:\n",
    "        print(f\"Error en los datos o formato: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error inesperado: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bb6185",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ruta_curva = r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\Curva Synplin Retail.xlsx\"\n",
    "ruta_calendario = r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\Calendario Apertura y Cierre 2025.xlsx\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafdb62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_curva = crear_dataframe(ruta_curva)\n",
    "df_curva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fd5d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar = crear_dataframe(ruta_calendario)\n",
    "df_calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b27a130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# código por parte  // VALIDADO QUE FUNCIONA BIEN\n",
    "df_calendario = df_calendar\n",
    "\n",
    "\n",
    "# Convertir columnas de fecha en df_calendario a datetime\n",
    "df_calendario['Fecha Inicio'] = pd.to_datetime(df_calendario['Fecha Inicio'])\n",
    "df_calendario['Fecha Fin'] = pd.to_datetime(df_calendario['Fecha Fin'])\n",
    "\n",
    "        # Identificar las columnas que representan los días en la curva de demanda\n",
    "columnas_dias_curva = [col for col in df_curva.columns if 'Día' in col]\n",
    "\n",
    "if not columnas_dias_curva:\n",
    "    raise ValueError(\"No se encontraron columnas de días en la curva de demanda. \"\n",
    "                             \"Asegúrese de que los nombres de las columnas contengan 'Día'.\")\n",
    "\n",
    "        # --- Estandarización de 'Año' y 'Ciclo' en df_curva ---\n",
    "if 'Año' in df_curva.columns:\n",
    "    df_curva['Año_str_temp'] = df_curva['Año'].astype(str).str.replace('FY', '', regex=False)\n",
    "    df_curva['Año_temp'] = pd.to_numeric(df_curva['Año_str_temp'], errors='coerce')\n",
    "    df_curva['Año'] = df_curva.apply(\n",
    "                lambda row: int(row['Año_temp']) + 2000 if pd.notna(row['Año_temp']) and len(str(int(row['Año_temp']))) <= 2 else int(row['Año']),\n",
    "                axis=1\n",
    "            )\n",
    "    df_curva.drop(columns=['Año_str_temp', 'Año_temp'], inplace=True, errors='ignore')\n",
    "    df_curva['Año'] = df_curva['Año'].astype(int)\n",
    "\n",
    "if 'Ciclo' in df_curva.columns:\n",
    "    df_curva['Ciclo'] = df_curva['Ciclo'].astype(str).str.replace('C', '', regex=False).str.lstrip('0')\n",
    "    df_curva['Ciclo'] = pd.to_numeric(df_curva['Ciclo'], errors='coerce').fillna(0).astype(int)\n",
    "        # --- FIN Estandarización ---\n",
    "\n",
    "        # --- Estandarización de 'Año' y 'Ciclo' en df_calendario (similar a df_curva si es necesario) ---\n",
    "if 'Año' in df_calendario.columns:\n",
    "    df_calendario['Año_str_temp'] = df_calendario['Año'].astype(str).str.replace('FY', '', regex=False)\n",
    "    df_calendario['Año_temp'] = pd.to_numeric(df_calendario['Año_str_temp'], errors='coerce')\n",
    "    df_calendario['Año'] = df_calendario.apply(\n",
    "                lambda row: int(row['Año_temp']) + 2000 if pd.notna(row['Año_temp']) and len(str(int(row['Año_temp']))) <= 2 else int(row['Año']),\n",
    "                axis=1\n",
    "            )\n",
    "    df_calendario.drop(columns=['Año_str_temp', 'Año_temp'], inplace=True, errors='ignore')\n",
    "    df_calendario['Año'] = df_calendario['Año'].astype(int)\n",
    "\n",
    "if 'Ciclo' in df_calendario.columns:\n",
    "    df_calendario['Ciclo'] = df_calendario['Ciclo'].astype(str).str.replace('C', '', regex=False).str.lstrip('0')\n",
    "    df_calendario['Ciclo'] = pd.to_numeric(df_calendario['Ciclo'], errors='coerce').fillna(0).astype(int)\n",
    "        # --- FIN Estandarización de calendario ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd59dc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame final para acumular los resultados\n",
    "df_resultado_final = pd.DataFrame(columns=[\n",
    "            'Año', 'Ciclo', 'Subciclo', 'Código_Kit', 'Código_Producto', 'Canal',\n",
    "            'Demanda', 'Día Ciclo', 'Demanda Día Ciclo'\n",
    "        ])\n",
    "\n",
    "fecha_hoy = datetime.now() # Ojo: Usa datetime.now() o una fecha fija para pruebas\n",
    "\n",
    "# Iterar sobre cada fila de df_demanda_retail\n",
    "for index, row in df_demanda_retail.iterrows():\n",
    "    año = row['Año']\n",
    "    ciclo = row['Ciclo']\n",
    "    subciclo = row['Subciclo']\n",
    "    demanda_total = row['Demanda']\n",
    "\n",
    "    # Buscar la curva de demanda correspondiente\n",
    "    curva_especifica = df_curva[\n",
    "        (df_curva['Año'] == año) &\n",
    "        (df_curva['Ciclo'] == ciclo) &\n",
    "        (df_curva['Subciclo'] == subciclo)\n",
    "        ]\n",
    "\n",
    "    # Buscar el calendario específico\n",
    "    calendario_especifico = df_calendario[\n",
    "        (df_calendario['Año'] == año) &\n",
    "        (df_calendario['Ciclo'] == ciclo) &\n",
    "        (df_calendario['Subciclo'] == subciclo)\n",
    "        ]\n",
    "    \n",
    "    if not curva_especifica.empty and not calendario_especifico.empty:\n",
    "        curva_especifica = curva_especifica.iloc[0]\n",
    "        calendario_especifico = calendario_especifico.iloc[0]\n",
    "\n",
    "        fecha_inicio_ciclo = calendario_especifico['Fecha Inicio']\n",
    "        fecha_fin_ciclo = calendario_especifico['Fecha Fin']\n",
    "        dias_curva = calendario_especifico['Dias curva'] # Asumiendo que 'Dias curva' está en el calendario\n",
    "\n",
    "        demanda_distribuida_por_dia = {}\n",
    "                \n",
    "        # Caso 1: fecha_hoy > fecha_fin_ciclo (Ciclo terminado)\n",
    "        if fecha_hoy > fecha_fin_ciclo:\n",
    "            for i in range(1, int(dias_curva) + 1):\n",
    "                col_dia_name = f'Día {i}'\n",
    "                porcentaje_dia = curva_especifica.get(col_dia_name, 0) # Usar .get para manejar posibles faltantes\n",
    "                demanda_distribuida_por_dia[i] = demanda_total * (porcentaje_dia / 100) # Asumir que porcentajes están en %\n",
    "                \n",
    "        # Caso 2: fecha_hoy < fecha_inicio_ciclo (Ciclo futuro)\n",
    "        elif fecha_hoy < fecha_inicio_ciclo:\n",
    "            for i in range(1, int(dias_curva) + 1):\n",
    "                col_dia_name = f'Día {i}'\n",
    "                porcentaje_dia = curva_especifica.get(col_dia_name, 0)\n",
    "                demanda_distribuida_por_dia[i] = demanda_total * (porcentaje_dia / 100)\n",
    "                \n",
    "        # Caso 3: fecha_inicio_ciclo <= fecha_hoy <= fecha_fin_ciclo (Ciclo en curso)\n",
    "        else:\n",
    "            dias_transcurridos = (fecha_hoy - fecha_inicio_ciclo).days + 1\n",
    "                    \n",
    "            # Calcular la demanda ya \"consumida\" hasta hoy\n",
    "            porcentaje_acumulado_pasado = 0\n",
    "            for i in range(1, dias_transcurridos): # Sumar porcentajes de días anteriores a hoy\n",
    "                col_dia_name = f'Día {i}'\n",
    "                porcentaje_dia = curva_especifica.get(col_dia_name, 0)\n",
    "                porcentaje_acumulado_pasado += porcentaje_dia\n",
    "\n",
    "            # La demanda pendiente es el total\n",
    "            demanda_pendiente = demanda_total\n",
    "\n",
    "            # Calcular la suma de los porcentajes de los días restantes\n",
    "            suma_porcentajes_pendientes = 0\n",
    "            for i in range(dias_transcurridos, int(dias_curva) + 1):\n",
    "                col_dia_name = f'Día {i}'\n",
    "                porcentaje_dia = curva_especifica.get(col_dia_name, 0)\n",
    "                suma_porcentajes_pendientes += porcentaje_dia\n",
    "\n",
    "            if suma_porcentajes_pendientes > 0:\n",
    "                # Distribuir la demanda pendiente en los días restantes manteniendo la proporción\n",
    "                for i in range(1, int(dias_curva) + 1):\n",
    "                    col_dia_name = f'Día {i}'\n",
    "                    if i < dias_transcurridos:\n",
    "                        # Días pasados tienen 0 demanda, se asume que ya se distribuyó\n",
    "                        demanda_distribuida_por_dia[i] = 0\n",
    "                    else:\n",
    "                        porcentaje_dia_original = curva_especifica.get(col_dia_name, 0)\n",
    "                        # Redistribuir la demanda pendiente\n",
    "                        demanda_distribuida_por_dia[i] = demanda_pendiente * (porcentaje_dia_original / suma_porcentajes_pendientes)\n",
    "            else:\n",
    "                # Si no hay días pendientes con porcentaje, la demanda pendiente se vuelve 0 para esos días\n",
    "                for i in range(1, int(dias_curva) + 1):\n",
    "                    demanda_distribuida_por_dia[i] = 0\n",
    "    else:\n",
    "        # Si no se encuentra curva o calendario, la demanda por día es 0\n",
    "        for i in range(1, int(curva_especifica.get('Dias curva', 0)) + 1 if not curva_especifica.empty else 1):\n",
    "            demanda_distribuida_por_dia[i] = 0 # Inicializar a 0 si no hay curva/calendario\n",
    "                \n",
    "    # Agregar las filas al DataFrame final\n",
    "    for dia_ciclo, demanda_dia_ciclo in demanda_distribuida_por_dia.items():\n",
    "        new_row = row.to_dict()\n",
    "        new_row['Día Ciclo'] = dia_ciclo\n",
    "        new_row['Demanda Día Ciclo'] = demanda_dia_ciclo\n",
    "        df_resultado_final = pd.concat([df_resultado_final, pd.DataFrame([new_row])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942db339",
   "metadata": {},
   "outputs": [],
   "source": [
    "calendario_especifico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c370ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_curva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fab30c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fb2900",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demanda_distribuida = distribuir_demanda(df_demanda_retail, ruta_curva, ruta_calendario)\n",
    "df_demanda_distribuida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cf4973",
   "metadata": {},
   "source": [
    "# Distribución demanda por dia de ciclo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c463347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versión 2 final\n",
    "import pandas as pd\n",
    "\n",
    "def distribuir_demanda(df_demanda_retail, ruta_curva_demanda): # Sirve para vol y retail\n",
    "    \"\"\"\n",
    "    Distribuye la demanda del dataframe df_demanda_retail en los días del ciclo\n",
    "    según los porcentajes definidos en la curva de demanda.\n",
    "    Maneja valores de porcentaje con coma decimal y símbolo '%'.\n",
    "    Estandariza los campos 'Año' y 'Ciclo' en df_curva para que coincidan con df_demanda_retail.\n",
    "    Mantiene los nombres originales de las columnas de días (ej. 'Día 1', 'Día 2').\n",
    "\n",
    "    Args:\n",
    "        df_demanda_retail (pd.DataFrame): DataFrame con los campos Año, Ciclo, Subciclo,\n",
    "                                        Código_Kit, Código_Producto, Canal y Demanda.\n",
    "        ruta_curva_demanda (str): Ruta al archivo Excel (.xlsx) o CSV que contiene la curva de demanda.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con los datos de df_demanda_retail y las columnas 'Día N'\n",
    "                      que contienen la demanda distribuida.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Cargar la curva de demanda.\n",
    "        if ruta_curva_demanda.endswith('.xlsx'):\n",
    "            df_curva = pd.read_excel(ruta_curva_demanda)\n",
    "        elif ruta_curva_demanda.endswith('.csv'):\n",
    "            df_curva = pd.read_csv(ruta_curva_demanda, decimal=',')\n",
    "        else:\n",
    "            raise ValueError(\"Formato de archivo no soportado. Por favor, use .xlsx o .csv\")\n",
    "\n",
    "        # Identificar las columnas que representan los días en la curva de demanda\n",
    "        columnas_dias_curva = [col for col in df_curva.columns if 'Día' in col]\n",
    "\n",
    "        if not columnas_dias_curva:\n",
    "            raise ValueError(\"No se encontraron columnas de días en la curva de demanda. \"\n",
    "                             \"Asegúrese de que los nombres de las columnas contengan 'Día'.\")\n",
    "\n",
    "        # --- Estandarización de 'Año' y 'Ciclo' en df_curva ---\n",
    "        if 'Año' in df_curva.columns:\n",
    "            df_curva['Año_str_temp'] = df_curva['Año'].astype(str).str.replace('FY', '', regex=False)\n",
    "            df_curva['Año_temp'] = pd.to_numeric(df_curva['Año_str_temp'], errors='coerce')\n",
    "            df_curva['Año'] = df_curva.apply(\n",
    "                lambda row: int(row['Año_temp']) + 2000 if pd.notna(row['Año_temp']) and len(str(int(row['Año_temp']))) <= 2 else int(row['Año']),\n",
    "                axis=1\n",
    "            )\n",
    "            df_curva.drop(columns=['Año_str_temp', 'Año_temp'], inplace=True, errors='ignore')\n",
    "            df_curva['Año'] = df_curva['Año'].astype(int)\n",
    "\n",
    "        if 'Ciclo' in df_curva.columns:\n",
    "            df_curva['Ciclo'] = df_curva['Ciclo'].astype(str).str.replace('C', '', regex=False).str.lstrip('0')\n",
    "            df_curva['Ciclo'] = pd.to_numeric(df_curva['Ciclo'], errors='coerce').fillna(0).astype(int)\n",
    "        # --- FIN Estandarización ---\n",
    "\n",
    "        # --- APLICAR LA LIMPIEZA Y CONVERSIÓN DE PORCENTAJES ---\n",
    "        for col in columnas_dias_curva:\n",
    "            df_curva[col] = (\n",
    "                df_curva[col]\n",
    "                .astype(str)\n",
    "                .str.replace('%', '', regex=False)\n",
    "                .str.replace(',', '.', regex=False)\n",
    "            )\n",
    "            df_curva[col] = pd.to_numeric(df_curva[col], errors='coerce')\n",
    "        # --- FIN Limpieza de Porcentajes ---\n",
    "\n",
    "        # --- Preparar el DataFrame de resultado ---\n",
    "        # Obtener el conjunto único de todas las columnas 'Día N' que existen en df_curva\n",
    "        all_day_columns = sorted([col for col in df_curva.columns if 'Día' in col and col.replace('Día ', '').isdigit()],\n",
    "                                 key=lambda x: int(x.replace('Día ', '')))\n",
    "        \n",
    "        # Crear un df temporal con las columnas de demanda retail + todas las columnas de días inicializadas en 0\n",
    "        df_resultado_base = df_demanda_retail.copy()\n",
    "        for col_dia in all_day_columns:\n",
    "            df_resultado_base[col_dia] = 0.0 # Inicializar con float\n",
    "\n",
    "\n",
    "        # Iterar sobre cada fila de df_demanda_retail (que ahora es df_resultado_base)\n",
    "        for index, row in df_resultado_base.iterrows():\n",
    "            # Asegurarse de que 'Año', 'Ciclo', 'Subciclo' de df_demanda_retail sean int para la comparación\n",
    "            año = row['Año']\n",
    "            ciclo = row['Ciclo']\n",
    "            subciclo = row['Subciclo']\n",
    "            demanda_total = row['Demanda']\n",
    "\n",
    "            # Buscar la curva de demanda correspondiente a Año, Ciclo y Subciclo\n",
    "            curva_especifica = df_curva[\n",
    "                (df_curva['Año'] == año) &\n",
    "                (df_curva['Ciclo'] == ciclo) &\n",
    "                (df_curva['Subciclo'] == subciclo)\n",
    "            ]\n",
    "\n",
    "            if not curva_especifica.empty:\n",
    "                curva_especifica = curva_especifica.iloc[0] # Tomar la primera coincidencia\n",
    "                \n",
    "                # Para cada columna de día identificada en la curva de demanda\n",
    "                for col_dia_name in columnas_dias_curva:\n",
    "                    try:\n",
    "                        valor_porcentaje_curva = curva_especifica[col_dia_name]\n",
    "                        porcentaje_dia = valor_porcentaje_curva if pd.notna(valor_porcentaje_curva) else 0\n",
    "                        demanda_distribuida = porcentaje_dia\n",
    "\n",
    "                        # Asignar la demanda distribuida directamente a la columna 'Día N'\n",
    "                        if col_dia_name in df_resultado_base.columns:\n",
    "                            df_resultado_base.at[index, col_dia_name] = demanda_distribuida\n",
    "                        # else: # No es necesario un else aquí, ya que se inicializan todas las columnas\n",
    "                        #     pass # Si la columna no existe, no se hace nada y se queda con el 0 inicial\n",
    "\n",
    "                    except KeyError:\n",
    "                        # Esto ocurriría si col_dia_name existe en columnas_dias_curva pero no en curva_especifica.\n",
    "                        # Dado cómo se construye columnas_dias_curva, es poco probable para un 'Día N' válido.\n",
    "                        # Se mantendría el 0 inicializado en df_resultado_base.\n",
    "                        pass\n",
    "                    except Exception:\n",
    "                        # Cualquier otro error durante el cálculo de un día, mantiene el 0 inicializado\n",
    "                        pass\n",
    "\n",
    "            # else: # No es necesario un else aquí, las columnas de días ya están en 0 en df_resultado_base\n",
    "                # Si no se encuentra curva, las columnas de días ya están inicializadas a 0\n",
    "                # pass\n",
    "\n",
    "        return df_resultado_base # Retornar el DataFrame base modificado\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: El archivo no fue encontrado en la ruta '{ruta_curva_demanda}'.\")\n",
    "        return pd.DataFrame()\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Falta una columna esperada en uno de los DataFrames. Detalle: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error inesperado: {e}\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c131334",
   "metadata": {},
   "outputs": [],
   "source": [
    "demanda_retail = distribuir_demanda(df_demanda_retail, r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\Curva Synplin Retail.xlsx\")\n",
    "demanda_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66990cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def transformar_df_dias(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma un DataFrame de Pandas, convirtiendo columnas de días\n",
    "    ('Día 1', 'Día 2', ..., 'Día N') en una sola columna 'Dia_Ciclo'\n",
    "    y los valores correspondientes en una columna 'Valor_Dia_Ciclo'.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): El DataFrame original con las columnas de días.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: El DataFrame transformado con una columna 'Dia_Ciclo'\n",
    "                      y los valores de demanda por día en 'Valor_Dia_Ciclo'.\n",
    "    \"\"\"\n",
    "    # Identifica las columnas que contienen los datos de los días\n",
    "    columnas_dias = [col for col in df.columns if col.startswith('Día ')]\n",
    "\n",
    "    # Identifica las columnas que no se van a \"derretir\" (mantener como identificadores)\n",
    "    columnas_id = ['Año', 'Ciclo', 'Subciclo', 'Código_Kit', 'Código_Producto', 'Canal', 'Demanda']\n",
    "\n",
    "    # Utiliza pd.melt para \"despivotar\" el DataFrame\n",
    "    df_transformado = pd.melt(df,\n",
    "                              id_vars=columnas_id,\n",
    "                              value_vars=columnas_dias,\n",
    "                              var_name='Dia_Ciclo',\n",
    "                              value_name='Valor_Dia_Ciclo') # ¡Aquí está el cambio!\n",
    "\n",
    "    # Extraer el número del día de la columna 'Dia_Ciclo' (ej. 'Día 1' -> 1)\n",
    "    # y convertirlo a tipo entero para facilitar operaciones futuras\n",
    "    df_transformado['Dia_Ciclo'] = df_transformado['Dia_Ciclo'].str.replace('Día ', '').astype(int)\n",
    "\n",
    "    # Reordenar las columnas para que 'Dia_Ciclo' y 'Valor_Dia_Ciclo'\n",
    "    # estén en el orden deseado\n",
    "    orden_columnas = ['Año', 'Ciclo', 'Subciclo', 'Código_Kit', 'Código_Producto',\n",
    "                      'Canal', 'Demanda', 'Dia_Ciclo', 'Valor_Dia_Ciclo']\n",
    "    df_transformado = df_transformado[orden_columnas]\n",
    "\n",
    "    return df_transformado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de877c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "demanda_retail_curva = transformar_df_dias(demanda_retail)\n",
    "demanda_retail_curva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdec477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este método funciona para clasificar los días del ciclo calendario\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "def incorporar_dia_calendario(\n",
    "    demanda_df: pd.DataFrame,\n",
    "    ruta_calendario_excel: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Incorpora el Dia_Calendario calculado al DataFrame de demanda_retail_curva,\n",
    "    leyendo el calendario desde un archivo Excel (.xlsx) y determinando la fecha\n",
    "    basada en el Dia_Ciclo dentro del rango Fecha Inicio y Fecha Fin.\n",
    "    Asume que el archivo Excel del calendario tiene una sola hoja.\n",
    "\n",
    "    Args:\n",
    "        demanda_df (pd.DataFrame): DataFrame 'demanda_retail_curva' con columnas\n",
    "                                   Año, Ciclo, Subciclo, Dia_Ciclo.\n",
    "                                   (Asumiendo 'Año' en formato 'FYxx', 'Ciclo' en 'Cxx', 'Subciclo' en 'A/B/etc.')\n",
    "        ruta_calendario_excel (str): La ruta completa al archivo Excel del calendario.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un nuevo DataFrame con Dia_Calendario, Dia_Inicio y Dia_Fin\n",
    "                      incorporados, en formato dd-mm-yyyy.\n",
    "    \"\"\"\n",
    "    print(\"--- DEBUG: Paso 1 - Inicio de la función ---\")\n",
    "    print(\"demanda_df original head:\\n\", demanda_df.head())\n",
    "    print(\"demanda_df dtypes:\\n\", demanda_df.dtypes)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    try:\n",
    "        calendario_df_raw = pd.read_excel(ruta_calendario_excel)\n",
    "        print(\"--- DEBUG: Paso 2 - calendario_df_raw cargado ---\")\n",
    "        print(\"calendario_df_raw head:\\n\", calendario_df_raw.head())\n",
    "        print(\"calendario_df_raw dtypes:\\n\", calendario_df_raw.dtypes)\n",
    "        print(\"\\n\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: El archivo Excel no se encontró en la ruta: {ruta_calendario_excel}\")\n",
    "        return demanda_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo Excel del calendario: {e}\")\n",
    "        return demanda_df\n",
    "\n",
    "    # --- Preprocesamiento del calendario_df ---\n",
    "    calendario_df = calendario_df_raw.copy()\n",
    "\n",
    "    # Convertir 'Año'\n",
    "    if 'Año' in calendario_df.columns and calendario_df['Año'].dtype == object and \\\n",
    "       calendario_df['Año'].astype(str).str.startswith('FY').any():\n",
    "        calendario_df['Año'] = calendario_df['Año'].astype(str).str.replace('FY', '20').astype(int)\n",
    "    else:\n",
    "        calendario_df['Año'] = calendario_df['Año'].astype(int)\n",
    "\n",
    "    # Convertir 'Ciclo'\n",
    "    if 'Ciclo' in calendario_df.columns and calendario_df['Ciclo'].dtype == object and \\\n",
    "       calendario_df['Ciclo'].astype(str).str.startswith('C').any():\n",
    "        calendario_df['Ciclo'] = calendario_df['Ciclo'].astype(str).str.replace('C', '').astype(int)\n",
    "    else:\n",
    "        calendario_df['Ciclo'] = calendario_df['Ciclo'].astype(int)\n",
    "\n",
    "    # Asegurarse de que 'Subciclo' sea string y limpiar espacios\n",
    "    if 'Subciclo' in calendario_df.columns:\n",
    "        calendario_df['Subciclo'] = calendario_df['Subciclo'].astype(str).str.strip()\n",
    "\n",
    "    # Convertir 'Fecha Inicio' y 'Fecha Fin' a tipo datetime\n",
    "    calendario_df['Fecha Inicio'] = pd.to_datetime(calendario_df['Fecha Inicio'], errors='coerce')\n",
    "    calendario_df['Fecha Fin'] = pd.to_datetime(calendario_df['Fecha Fin'], errors='coerce')\n",
    "\n",
    "    # Calcular la duración del ciclo en días (inclusive)\n",
    "    calendario_df['Duracion_Real_Dias'] = (calendario_df['Fecha Fin'] - calendario_df['Fecha Inicio']).dt.days + 1\n",
    "\n",
    "    print(\"--- DEBUG: Paso 3 - calendario_df preprocesado ---\")\n",
    "    print(\"calendario_df head:\\n\", calendario_df.head())\n",
    "    print(\"calendario_df dtypes:\\n\", calendario_df.dtypes)\n",
    "    # Verificar NaT en fechas\n",
    "    print(\"NaT en Fecha Inicio (calendario_df):\", calendario_df['Fecha Inicio'].isna().sum())\n",
    "    print(\"NaT en Fecha Fin (calendario_df):\", calendario_df['Fecha Fin'].isna().sum())\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "    calendario_info = calendario_df[['Año', 'Ciclo', 'Subciclo', 'Fecha Inicio', 'Fecha Fin', 'Duracion_Real_Dias']].copy()\n",
    "\n",
    "    demanda_df_procesado = demanda_df.copy()\n",
    "\n",
    "    # Asegurar que las columnas de fusión en demanda_df_procesado tengan el mismo tipo\n",
    "    demanda_df_procesado['Año'] = demanda_df_procesado['Año'].astype(int)\n",
    "    demanda_df_procesado['Ciclo'] = demanda_df_procesado['Ciclo'].astype(int)\n",
    "    demanda_df_procesado['Subciclo'] = demanda_df_procesado['Subciclo'].astype(str).str.strip()\n",
    "\n",
    "\n",
    "    print(\"--- DEBUG: Paso 4 - demanda_df_procesado antes de la fusión ---\")\n",
    "    print(\"demanda_df_procesado head:\\n\", demanda_df_procesado.head())\n",
    "    print(\"demanda_df_procesado dtypes:\\n\", demanda_df_procesado.dtypes)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Fusionar los DataFrames para obtener las fechas de inicio y fin para cada fila de demanda\n",
    "    demanda_con_fechas_ciclo = pd.merge(\n",
    "        demanda_df_procesado,\n",
    "        calendario_info,\n",
    "        on=['Año', 'Ciclo', 'Subciclo'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    print(\"--- DEBUG: Paso 5 - demanda_con_fechas_ciclo después de la fusión ---\")\n",
    "    print(\"demanda_con_fechas_ciclo head:\\n\", demanda_con_fechas_ciclo.head(10)) # Muestra más filas\n",
    "    print(\"demanda_con_fechas_ciclo dtypes:\\n\", demanda_con_fechas_ciclo.dtypes)\n",
    "    # Verificar cuántas filas tienen NaN en Fecha Inicio o Fecha Fin después de la fusión\n",
    "    print(\"Filas con NaN en 'Fecha Inicio' o 'Fecha Fin' después de fusión:\",\n",
    "          demanda_con_fechas_ciclo[['Fecha Inicio', 'Fecha Fin', 'Duracion_Real_Dias']].isna().any(axis=1).sum())\n",
    "    # Muestra las filas que no encontraron match\n",
    "    print(\"Filas sin match en calendario:\\n\", demanda_con_fechas_ciclo[demanda_con_fechas_ciclo['Fecha Inicio'].isna()])\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Función para calcular Dia_Calendario, Dia_Inicio y Dia_Fin\n",
    "    def calcular_fechas_para_fila(row):\n",
    "        fecha_inicio_ciclo = row['Fecha Inicio']\n",
    "        fecha_fin_ciclo = row['Fecha Fin']\n",
    "        dia_ciclo_demanda = row['Dia_Ciclo']\n",
    "        duracion_ciclo = row['Duracion_Real_Dias']\n",
    "\n",
    "        dia_calendario = pd.NaT\n",
    "        dia_inicio_formato = None\n",
    "        dia_fin_formato = None\n",
    "\n",
    "        if pd.notna(fecha_inicio_ciclo) and pd.notna(fecha_fin_ciclo) and pd.notna(dia_ciclo_demanda):\n",
    "            # Asegurarse de que dia_ciclo_demanda sea un entero antes de la operación\n",
    "            try:\n",
    "                dia_ciclo_demanda_int = int(dia_ciclo_demanda)\n",
    "            except ValueError:\n",
    "                return pd.Series({\n",
    "                    'Dia_Calendario': pd.NaT,\n",
    "                    'Dia_Inicio': None,\n",
    "                    'Dia_Fin': None\n",
    "                })\n",
    "\n",
    "            calculated_date = fecha_inicio_ciclo + timedelta(days=dia_ciclo_demanda_int - 1)\n",
    "\n",
    "            # Validar rango\n",
    "            if fecha_inicio_ciclo <= calculated_date <= fecha_fin_ciclo and \\\n",
    "               1 <= dia_ciclo_demanda_int <= duracion_ciclo:\n",
    "                dia_calendario = calculated_date\n",
    "            # else: dia_calendario ya es pd.NaT\n",
    "\n",
    "            if pd.notna(fecha_inicio_ciclo):\n",
    "                dia_inicio_formato = fecha_inicio_ciclo.strftime('%d-%m-%Y')\n",
    "            if pd.notna(fecha_fin_ciclo):\n",
    "                dia_fin_formato = fecha_fin_ciclo.strftime('%d-%m-%Y')\n",
    "\n",
    "        return pd.Series({\n",
    "            'Dia_Calendario': dia_calendario,\n",
    "            'Dia_Inicio': dia_inicio_formato,\n",
    "            'Dia_Fin': dia_fin_formato\n",
    "        })\n",
    "\n",
    "    # Aplicar la función a cada fila del DataFrame\n",
    "    fechas_calculadas = demanda_con_fechas_ciclo.apply(calcular_fechas_para_fila, axis=1)\n",
    "\n",
    "    print(\"--- DEBUG: Paso 6 - fechas_calculadas después de apply ---\")\n",
    "    print(\"fechas_calculadas head:\\n\", fechas_calculadas.head(10))\n",
    "    print(\"NaT en Dia_Calendario (fechas_calculadas):\", fechas_calculadas['Dia_Calendario'].isna().sum())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Unir las nuevas columnas al DataFrame original\n",
    "    demanda_retail_curva_calendario = pd.concat([demanda_con_fechas_ciclo, fechas_calculadas], axis=1)\n",
    "\n",
    "    # Eliminar las columnas temporales\n",
    "    demanda_retail_curva_calendario = demanda_retail_curva_calendario.drop(columns=[\n",
    "        'Fecha Inicio', 'Fecha Fin', 'Duracion_Real_Dias'\n",
    "    ])\n",
    "\n",
    "    # Reordenar las columnas\n",
    "    cols = demanda_retail_curva_calendario.columns.tolist()\n",
    "    new_date_cols = ['Dia_Calendario', 'Dia_Inicio', 'Dia_Fin']\n",
    "    existing_new_date_cols = [col for col in new_date_cols if col in cols]\n",
    "    for col in existing_new_date_cols:\n",
    "        if col in cols: # Comprobar de nuevo por si se eliminó en la iteración anterior (no debería ocurrir aquí)\n",
    "            cols.remove(col)\n",
    "\n",
    "    if 'Dia_Ciclo' in cols:\n",
    "        idx_dia_ciclo = cols.index('Dia_Ciclo')\n",
    "        for i, col in enumerate(existing_new_date_cols):\n",
    "            cols.insert(idx_dia_ciclo + 1 + i, col)\n",
    "    else:\n",
    "        cols.extend(existing_new_date_cols)\n",
    "\n",
    "    demanda_retail_curva_calendario = demanda_retail_curva_calendario[cols]\n",
    "\n",
    "    # Formatear la columna 'Dia_Calendario'\n",
    "    demanda_retail_curva_calendario['Dia_Calendario'] = \\\n",
    "        demanda_retail_curva_calendario['Dia_Calendario'].dt.strftime('%d-%m-%Y').replace({pd.NaT: None})\n",
    "\n",
    "    print(\"--- DEBUG: Paso 7 - DataFrame final (primeras 10 filas) ---\")\n",
    "    print(demanda_retail_curva_calendario.head(10))\n",
    "    print(\"NaT/None en Dia_Calendario (final):\", demanda_retail_curva_calendario['Dia_Calendario'].isna().sum())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return demanda_retail_curva_calendario\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd2de8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "demanda_retail_curva_calendario = incorporar_dia_calendario(demanda_retail_curva, r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\Calendario Apertura y Cierre 2025.xlsx\")\n",
    "demanda_retail_curva_calendario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989d49c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validar calendario\n",
    "demanda_retail_curva_calendario[demanda_retail_curva_calendario['Ciclo'] == 14][['Ciclo','Dia_Ciclo', 'Dia_Calendario', 'Dia_Inicio', 'Dia_Fin']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbfbddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def calcular_demanda_por_dia(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calcula las columnas 'Demanda_Dia_Ciclo' y 'Demanda_Dia_Calendario' en el DataFrame,\n",
    "    utilizando la fecha actual del sistema.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame de entrada con la información de demanda.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: El DataFrame con las nuevas columnas calculadas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtener la fecha de hoy y convertirla a un Timestamp de Pandas\n",
    "    # --- ¡CORRECCIÓN CLAVE AQUÍ! ---\n",
    "    fecha_hoy = pd.to_datetime(datetime.now().date())\n",
    "    print(f\"La fecha actual considerada para el cálculo es: {fecha_hoy.strftime('%Y-%m-%d')}\")\n",
    "    # --- FIN DE LA CORRECCIÓN ---\n",
    "\n",
    "\n",
    "    # Asegúrate de que las columnas de fecha sean tipo datetime y con el formato correcto\n",
    "    df['Dia_Inicio'] = pd.to_datetime(df['Dia_Inicio'], format='%d-%m-%Y', errors='coerce')\n",
    "    df['Dia_Fin'] = pd.to_datetime(df['Dia_Fin'], format='%d-%m-%Y', errors='coerce')\n",
    "    df['Dia_Calendario'] = pd.to_datetime(df['Dia_Calendario'], format='%d-%m-%Y', errors='coerce')\n",
    "\n",
    "    # 1. Calcular Demanda_Dia_Ciclo\n",
    "    df['Demanda_Dia_Ciclo'] = df['Demanda'] * df['Valor_Dia_Ciclo']\n",
    "\n",
    "    # 2. Inicializar Demanda_Dia_Calendario con Demanda_Dia_Ciclo\n",
    "    df['Demanda_Dia_Calendario'] = df['Demanda_Dia_Ciclo']\n",
    "\n",
    "    # Función auxiliar para aplicar la lógica de cálculo por grupo\n",
    "    def _calcular_demanda_grupo(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        group = group.copy() # Trabajar con una copia para evitar SettingWithCopyWarning\n",
    "\n",
    "        # Días desde fecha_hoy hasta Dia_Fin: Recalcular\n",
    "        # Ahora comparamos Timestamp con Timestamp, lo cual es compatible\n",
    "        mask_dias_futuros = (group['Dia_Calendario'] >= fecha_hoy) & \\\n",
    "                            (group['Dia_Calendario'] <= group['Dia_Fin'].iloc[0])\n",
    "\n",
    "\n",
    "        if mask_dias_futuros.any():\n",
    "            # Demanda acumulada hasta el día anterior a fecha_hoy\n",
    "            mask_dias_pasados = (group['Dia_Calendario'] < fecha_hoy)\n",
    "            demanda_acumulada_pasada = group.loc[mask_dias_pasados, 'Demanda_Dia_Ciclo'].sum()\n",
    "\n",
    "            # Demanda total del ciclo (asumimos que es la misma para todo el grupo)\n",
    "            demanda_total_ciclo = group['Demanda'].iloc[0]\n",
    "\n",
    "            # Demanda restante a distribuir\n",
    "            demanda_restante_a_distribuir = demanda_total_ciclo - demanda_acumulada_pasada\n",
    "\n",
    "            # Suma de los 'Valor_Dia_Ciclo' para los días futuros dentro del grupo\n",
    "            suma_valor_dia_ciclo_futuro = group.loc[mask_dias_futuros, 'Valor_Dia_Ciclo'].sum()\n",
    "\n",
    "            if suma_valor_dia_ciclo_futuro > 0:\n",
    "                # Recalcular Valor_Dia_Ciclo y luego Demanda_Dia_Calendario para estos días\n",
    "                recalibrated_valor_dia_ciclo = group.loc[mask_dias_futuros, 'Valor_Dia_Ciclo'] / suma_valor_dia_ciclo_futuro\n",
    "                group.loc[mask_dias_futuros, 'Demanda_Dia_Calendario'] = demanda_restante_a_distribuir * recalibrated_valor_dia_ciclo\n",
    "            else:\n",
    "                # Si no hay Valor_Dia_Ciclo en el futuro, no hay nada que distribuir en esa lógica.\n",
    "                # Se mantendrá la Demanda_Dia_Ciclo original para esos días.\n",
    "                pass\n",
    "        \n",
    "        return group\n",
    "\n",
    "    # Aplicar la función por grupo\n",
    "    df_resultado = df.groupby(['Año', 'Ciclo', 'Subciclo', 'Código_Kit', 'Código_Producto', 'Canal'], group_keys=False).apply(_calcular_demanda_grupo)\n",
    "\n",
    "    # Asegurarse de que el orden de las columnas se mantenga o se añadan al final\n",
    "    return df_resultado.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d920ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta # Importar timedelta para sumar días\n",
    "\n",
    "def calcular_demanda_por_dia(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calcula las columnas 'Demanda_Dia_Ciclo' y 'Demanda_Dia_Calendario' en el DataFrame,\n",
    "    utilizando la fecha actual del sistema.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame de entrada con la información de demanda.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: El DataFrame con las nuevas columnas calculadas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtener la fecha de hoy y convertirla a un Timestamp de Pandas\n",
    "    fecha_hoy = pd.to_datetime(datetime.now().date())\n",
    "    print(f\"La fecha actual considerada para el cálculo es: {fecha_hoy.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "    # Asegúrate de que las columnas de fecha sean tipo datetime y con el formato correcto\n",
    "    df['Dia_Inicio'] = pd.to_datetime(df['Dia_Inicio'], format='%d-%m-%Y', errors='coerce')\n",
    "    df['Dia_Fin'] = pd.to_datetime(df['Dia_Fin'], format='%d-%m-%Y', errors='coerce')\n",
    "    df['Dia_Calendario'] = pd.to_datetime(df['Dia_Calendario'], format='%d-%m-%Y', errors='coerce')\n",
    "\n",
    "    # 1. Calcular Demanda_Dia_Ciclo (Esto se mantiene igual)\n",
    "    df['Demanda_Dia_Ciclo'] = df['Demanda'] * df['Valor_Dia_Ciclo']\n",
    "\n",
    "    # 2. Inicializar Demanda_Dia_Calendario con Demanda_Dia_Ciclo\n",
    "    # Para los días que NO caen en el rango de redistribución, este será el valor final.\n",
    "    df['Demanda_Dia_Calendario'] = df['Demanda_Dia_Ciclo']\n",
    "\n",
    "    # Función auxiliar para aplicar la lógica de cálculo por grupo\n",
    "    def _calcular_demanda_grupo(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        group = group.copy() # Trabajar con una copia para evitar SettingWithCopyWarning\n",
    "\n",
    "        # Aseguramos que Dia_Fin para el grupo se tome correctamente\n",
    "        dia_inicio_grupo = group['Dia_Inicio'].iloc[0]\n",
    "        dia_fin_grupo = group['Dia_Fin'].iloc[0]\n",
    "        demanda_total_ciclo = group['Demanda'].iloc[0]\n",
    "\n",
    "        # Verificar si la fecha de hoy cae dentro del rango [Dia_Inicio, Dia_Fin] del ciclo\n",
    "        if fecha_hoy >= dia_inicio_grupo and fecha_hoy <= dia_fin_grupo:\n",
    "            # 1. Calcular la demanda ya \"consumida\" hasta el día anterior a hoy\n",
    "            # Los días anteriores a fecha_hoy mantienen su Demanda_Dia_Ciclo original.\n",
    "            mask_dias_pasados = (group['Dia_Calendario'] < fecha_hoy)\n",
    "            demanda_acumulada_pasada = group.loc[mask_dias_pasados, 'Demanda_Dia_Ciclo'].sum()\n",
    "\n",
    "            # Asignar los valores de los días pasados a Demanda_Dia_Calendario\n",
    "            group.loc[mask_dias_pasados, 'Demanda_Dia_Calendario'] = group.loc[mask_dias_pasados, 'Demanda_Dia_Ciclo']\n",
    "\n",
    "            # 2. Calcular la demanda restante a distribuir\n",
    "            demanda_restante_a_distribuir = demanda_total_ciclo - demanda_acumulada_pasada\n",
    "\n",
    "            # 3. Identificar los días futuros (desde mañana hasta Dia_Fin)\n",
    "            fecha_manana = fecha_hoy + timedelta(days=1)\n",
    "            mask_dias_futuros = (group['Dia_Calendario'] >= fecha_manana) & \\\n",
    "                                (group['Dia_Calendario'] <= dia_fin_grupo)\n",
    "\n",
    "            # Suma de los Valor_Dia_Ciclo para los días futuros originales\n",
    "            suma_valor_dia_ciclo_futuro_original = group.loc[mask_dias_futuros, 'Valor_Dia_Ciclo'].sum()\n",
    "\n",
    "            if suma_valor_dia_ciclo_futuro_original > 0:\n",
    "                # Recalcular Valor_Dia_Ciclo para los días futuros\n",
    "                # Los nuevos porcentajes deben sumar 1 (100%) en este rango\n",
    "                recalibrated_valor_dia_ciclo = group.loc[mask_dias_futuros, 'Valor_Dia_Ciclo'] / suma_valor_dia_ciclo_futuro_original\n",
    "\n",
    "                # 4. Distribuir la demanda restante entre los días futuros\n",
    "                group.loc[mask_dias_futuros, 'Demanda_Dia_Calendario'] = demanda_restante_a_distribuir * recalibrated_valor_dia_ciclo\n",
    "            else:\n",
    "                # Si no hay días futuros o sus Valor_Dia_Ciclo suman 0,\n",
    "                # la demanda restante no se distribuye en esos días.\n",
    "                # En este caso, la Demanda_Dia_Calendario para esos días futuros seguirá siendo\n",
    "                # la inicializada (Demanda_Dia_Ciclo), que podría ser cero si Valor_Dia_Ciclo es cero.\n",
    "                # O si toda la demanda ya se consumió y no hay nada que distribuir, Demanda_Dia_Calendario\n",
    "                # para los días futuros será 0.\n",
    "                group.loc[mask_dias_futuros, 'Demanda_Dia_Calendario'] = 0 # No hay Valor_Dia_Ciclo para distribuir.\n",
    "\n",
    "            # 5. Manejar el 'Dia_Calendario' que corresponde a 'fecha_hoy'\n",
    "            # Si fecha_hoy coincide con un Dia_Calendario, este día no es \"pasado\" ni \"futuro\" para la redistribución.\n",
    "            # Según tu descripción original, \"Los días anteriores a hoy mantendrán la Demanda_Dia_Ciclo.\"\n",
    "            # Y la redistribución es \"desde el día de hoy hasta el día_fin\".\n",
    "            # Esto implica que el día de hoy puede ser el inicio de la redistribución de lo restante.\n",
    "            # Sin embargo, tu punto \"Los días anteriores a hoy mantendrán la Demanda_Dia_Ciclo\" es claro.\n",
    "            # La nueva especificación dice \"demanda_actual considera hasta fecha_hoy. Luego, esta demanda_actual debe distribuirse entre el día de mañana y fecha_fin\".\n",
    "            # Esto significa que el día 'fecha_hoy' no entra en la redistribución de la 'demanda_actual'.\n",
    "            # Por lo tanto, el día de hoy, si existe en el grupo, debería mantener su Demanda_Dia_Ciclo original.\n",
    "            mask_hoy = (group['Dia_Calendario'] == fecha_hoy)\n",
    "            group.loc[mask_hoy, 'Demanda_Dia_Calendario'] = group.loc[mask_hoy, 'Demanda_Dia_Ciclo']\n",
    "\n",
    "\n",
    "        # Para los casos donde fecha_hoy está fuera del rango [Dia_Inicio, Dia_Fin]\n",
    "        # (fecha_hoy > dia_fin_grupo o fecha_hoy < dia_inicio_grupo),\n",
    "        # Demanda_Dia_Calendario ya fue inicializada con Demanda_Dia_Ciclo y se mantiene así.\n",
    "\n",
    "        return group\n",
    "\n",
    "    # Aplicar la función por grupo\n",
    "    df_resultado = df.groupby(['Año', 'Ciclo', 'Subciclo', 'Código_Kit', 'Código_Producto', 'Canal'], group_keys=False).apply(_calcular_demanda_grupo)\n",
    "\n",
    "    # Asegurarse de que el orden de las columnas se mantenga o se añadan al final\n",
    "    return df_resultado.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d0f913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versión de ajustada con condiciones de borde // FUNCIONA BIEN DEJAR\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def calcular_demanda_por_dia(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calcula las columnas 'Demanda_Dia_Ciclo', 'Demanda_Dia_Calendario' y 'Valor_Dia_Ciclo_Calendario'\n",
    "    en el DataFrame, utilizando la fecha actual del sistema.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame de entrada con la información de demanda.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: El DataFrame con las nuevas columnas calculadas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtener la fecha de hoy y convertirla a un Timestamp de Pandas\n",
    "    fecha_hoy = pd.to_datetime(datetime.now().date())\n",
    "    print(f\"La fecha actual considerada para el cálculo es: {fecha_hoy.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "    # Asegúrate de que las columnas de fecha sean tipo datetime y con el formato correcto\n",
    "    df['Dia_Inicio'] = pd.to_datetime(df['Dia_Inicio'], format='%d-%m-%Y', errors='coerce')\n",
    "    df['Dia_Fin'] = pd.to_datetime(df['Dia_Fin'], format='%d-%m-%Y', errors='coerce')\n",
    "    df['Dia_Calendario'] = pd.to_datetime(df['Dia_Calendario'], format='%d-%m-%Y', errors='coerce')\n",
    "\n",
    "    # 1. Calcular Demanda_Dia_Ciclo (Esto se mantiene igual)\n",
    "    df['Demanda_Dia_Ciclo'] = df['Demanda'] * df['Valor_Dia_Ciclo']\n",
    "\n",
    "    # 2. Inicializar Demanda_Dia_Calendario con Demanda_Dia_Ciclo y Valor_Dia_Ciclo_Calendario con Valor_Dia_Ciclo\n",
    "    df['Demanda_Dia_Calendario'] = df['Demanda_Dia_Ciclo']\n",
    "    df['Valor_Dia_Ciclo_Calendario'] = df['Valor_Dia_Ciclo'] # Inicializar con el valor original\n",
    "\n",
    "    # Función auxiliar para aplicar la lógica de cálculo por grupo\n",
    "    def _calcular_demanda_grupo(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        group = group.copy()\n",
    "\n",
    "        dia_inicio_grupo = group['Dia_Inicio'].iloc[0]\n",
    "        dia_fin_grupo = group['Dia_Fin'].iloc[0]\n",
    "        demanda_total_ciclo = group['Demanda'].iloc[0]\n",
    "\n",
    "        # Condición principal: Si fecha_hoy está dentro del rango del ciclo [Dia_Inicio, Dia_Fin]\n",
    "        if fecha_hoy >= dia_inicio_grupo and fecha_hoy <= dia_fin_grupo:\n",
    "            # Los días hasta e incluyendo fecha_hoy mantienen Demanda_Dia_Ciclo como Demanda_Dia_Calendario\n",
    "            mask_dias_hasta_hoy = (group['Dia_Calendario'] <= fecha_hoy)\n",
    "            group.loc[mask_dias_hasta_hoy, 'Demanda_Dia_Calendario'] = group.loc[mask_dias_hasta_hoy, 'Demanda_Dia_Ciclo']\n",
    "            group.loc[mask_dias_hasta_hoy, 'Valor_Dia_Ciclo_Calendario'] = group.loc[mask_dias_hasta_hoy, 'Valor_Dia_Ciclo']\n",
    "\n",
    "            # Suma de la demanda hasta fecha_hoy (inclusive)\n",
    "            demanda_consumida_hasta_hoy = group.loc[mask_dias_hasta_hoy, 'Demanda_Dia_Calendario'].sum()\n",
    "\n",
    "            # Caso especial: Si fecha_hoy es el último día del ciclo (fecha_fin)\n",
    "            if fecha_hoy == dia_fin_grupo:\n",
    "                # No hay demanda pendiente para distribuir, se habrá \"cumplido\" el 100% de la demanda hasta hoy.\n",
    "                # Ya los valores de Demanda_Dia_Calendario y Valor_Dia_Ciclo_Calendario fueron establecidos arriba.\n",
    "                pass\n",
    "            else:\n",
    "                # Calcular la demanda pendiente a distribuir (desde mañana hasta fecha_fin)\n",
    "                demanda_pendiente_a_distribuir = demanda_total_ciclo - demanda_consumida_hasta_hoy\n",
    "                \n",
    "                # Asegurarse de que la demanda pendiente no sea negativa\n",
    "                demanda_pendiente_a_distribuir = max(0, demanda_pendiente_a_distribuir)\n",
    "\n",
    "                # Días futuros: desde fecha_hoy + 1 hasta fecha_fin\n",
    "                fecha_manana = fecha_hoy + timedelta(days=1)\n",
    "                mask_dias_futuros = (group['Dia_Calendario'] >= fecha_manana) & \\\n",
    "                                    (group['Dia_Calendario'] <= dia_fin_grupo)\n",
    "\n",
    "                # Suma de los Valor_Dia_Ciclo ORIGINALES para los días futuros\n",
    "                suma_valor_dia_ciclo_futuro_original = group.loc[mask_dias_futuros, 'Valor_Dia_Ciclo'].sum()\n",
    "\n",
    "                if suma_valor_dia_ciclo_futuro_original > 0:\n",
    "                    # Recalcular Valor_Dia_Ciclo_Calendario para los días futuros\n",
    "                    # Los nuevos porcentajes deben sumar 1 (100%) para la demanda pendiente\n",
    "                    recalibrated_valor_dia_ciclo = group.loc[mask_dias_futuros, 'Valor_Dia_Ciclo'] / suma_valor_dia_ciclo_futuro_original\n",
    "                    group.loc[mask_dias_futuros, 'Valor_Dia_Ciclo_Calendario'] = recalibrated_valor_dia_ciclo\n",
    "\n",
    "                    # Distribuir la demanda pendiente\n",
    "                    group.loc[mask_dias_futuros, 'Demanda_Dia_Calendario'] = demanda_pendiente_a_distribuir * recalibrated_valor_dia_ciclo\n",
    "                else:\n",
    "                    # Si no hay días futuros con Valor_Dia_Ciclo > 0 para redistribuir,\n",
    "                    # la demanda pendiente no se distribuye. Estos días quedan en 0 si no tienen valor inicial.\n",
    "                    group.loc[mask_dias_futuros, 'Demanda_Dia_Calendario'] = 0\n",
    "                    group.loc[mask_dias_futuros, 'Valor_Dia_Ciclo_Calendario'] = 0\n",
    "        \n",
    "        # Casos donde fecha_hoy está fuera del rango [Dia_Inicio, Dia_Fin]\n",
    "        # (fecha_hoy > dia_fin_grupo o fecha_hoy < dia_inicio_grupo):\n",
    "        # En estos casos, Demanda_Dia_Calendario y Valor_Dia_Ciclo_Calendario\n",
    "        # ya fueron inicializados con los valores de Demanda_Dia_Ciclo y Valor_Dia_Ciclo,\n",
    "        # lo cual es el comportamiento deseado según tus reglas iniciales.\n",
    "\n",
    "        return group\n",
    "\n",
    "    # Aplicar la función por grupo\n",
    "    df_resultado = df.groupby(['Año', 'Ciclo', 'Subciclo', 'Código_Kit', 'Código_Producto', 'Canal'], group_keys=False).apply(_calcular_demanda_grupo)\n",
    "\n",
    "    # Asegurarse de que el orden de las columnas se mantenga o se añadan al final\n",
    "    return df_resultado.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29a949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "demanda_retail_dia_recalculada = calcular_demanda_por_dia(demanda_retail_curva_calendario)\n",
    "demanda_retail_dia_recalculada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec12587",
   "metadata": {},
   "outputs": [],
   "source": [
    "demanda_retail_dia_recalculada[\n",
    "    (demanda_retail_dia_recalculada['Ciclo'] == 10) & \n",
    "    (demanda_retail_dia_recalculada['Código_Producto'] == 168788)\n",
    "]\n",
    "\n",
    "demanda_retail_dia_recalculada[\n",
    "    (demanda_retail_dia_recalculada['Ciclo'] == 10) & \n",
    "    (demanda_retail_dia_recalculada['Código_Producto'] == 168788)\n",
    "].to_excel(r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\test_curva_retail.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23ca2a7",
   "metadata": {},
   "source": [
    "# VERSION COMPLETA FINAL UNIFICADA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1b7060",
   "metadata": {},
   "source": [
    "## Carga de demandas explotadas por canal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f139cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar SQL\n",
    "import re\n",
    "\n",
    "# Obtener credenciales SQL Server\n",
    "# Path to the file\n",
    "file_path = r\"C:\\Users\\Spider Build\\Downloads\\SQL2019.txt\"\n",
    "\n",
    "# Function to extract username and password\n",
    "def extract_credentials(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    username_match = re.search(r'username=\"([^\"]+)\"', content)\n",
    "    password_match = re.search(r'password=\"([^\"]+)\"', content)\n",
    "    \n",
    "    if username_match and password_match:\n",
    "        username = username_match.group(1)\n",
    "        password = password_match.group(1)\n",
    "        return username , password\n",
    "    else:\n",
    "        return \"Username or password not found in the file.\"\n",
    "\n",
    "# Llamar la función de credenciales\n",
    "username_sql, password_sql = extract_credentials(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576ecfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener dataframe productos desde SQL\n",
    "# 2. Realizar left join con la tabla productos de plan_chile\n",
    "# Método para obtener df desde SQL Server\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine import URL\n",
    "import time\n",
    "\n",
    "def sql_to_df(\n",
    "    database_name: str,\n",
    "    schema_name: str,\n",
    "    table_name: str = None,\n",
    "    query: str = None,\n",
    "    server_address: str = \"10.156.16.45\\SQL2019\",\n",
    "    driver_name: str = \"ODBC Driver 17 for SQL Server\",\n",
    "    username_sql: str = None,\n",
    "    password_sql: str = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lee datos de una tabla de SQL Server o ejecuta una consulta SQL y los carga en un DataFrame de pandas.\n",
    "\n",
    "    Este método establece una conexión con SQL Server utilizando SQLAlchemy y pyodbc.\n",
    "    Permite tanto la lectura de una tabla completa especificando 'table_name'\n",
    "    como la ejecución de una consulta SQL personalizada mediante el parámetro 'query'.\n",
    "    Maneja la construcción de la URL de conexión y proporciona un control básico de errores.\n",
    "\n",
    "    Args:\n",
    "        database_name (str): El nombre de la base de datos (ej. \"CustomerCare\").\n",
    "        schema_name (str): El nombre del esquema (ej. \"customer_care\").\n",
    "        table_name (str, optional): El nombre de la tabla a leer. Se requiere si 'query' no se proporciona.\n",
    "                                     Si se usa, la función construirá un SELECT * FROM.\n",
    "        query (str, optional): La consulta SQL a ejecutar. Se requiere si 'table_name' no se proporciona.\n",
    "                                Se ejecutará la consulta directamente.\n",
    "        server_address (str): Dirección del servidor SQL Server (por defecto \"10.156.16.46\\\\SQL2022\").\n",
    "        driver_name (str): Nombre del driver ODBC (por defecto \"ODBC Driver 17 for SQL Server\").\n",
    "        username_sql (str, optional): Nombre de usuario para la conexión SQL. Si es None,\n",
    "                                      se intentará la autenticación integrada o se usarán variables de entorno.\n",
    "        password_sql (str, optional): Contraseña para la conexión SQL. Si es None,\n",
    "                                      se intentará la autenticación integrada o se usarán variables de entorno.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un DataFrame de pandas con los datos leídos de SQL Server.\n",
    "                      Retorna un DataFrame vacío si ocurre un error o no se encuentran datos.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Si no se proporciona ni 'table_name' ni 'query'.\n",
    "    \"\"\"\n",
    "    if table_name is None and query is None:\n",
    "        raise ValueError(\"Debes proporcionar 'table_name' o 'query'.\")\n",
    "\n",
    "    connection_url = URL.create(\n",
    "        \"mssql+pyodbc\",\n",
    "        username=username_sql,\n",
    "        password=password_sql,\n",
    "        host=server_address,\n",
    "        database=database_name,\n",
    "        query={\n",
    "            \"driver\": driver_name,\n",
    "            # \"TrustServerCertificate\": \"yes\", # Descomentar si es necesario para certificados autofirmados/no verificados\n",
    "            # \"authentication\": \"ActiveDirectoryIntegrated\", # Descomentar si usas esta autenticación\n",
    "        },\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    try:\n",
    "        engine = create_engine(connection_url)\n",
    "\n",
    "        start_time = time.time()\n",
    "        if query:\n",
    "            df = pd.read_sql_query(sql=query, con=engine)\n",
    "            print(f\"Consulta SQL ejecutada y cargada exitosamente en {time.time() - start_time:.2f} segundos.\")\n",
    "        elif table_name:\n",
    "            full_table_name = f'\"{schema_name}\".\"{table_name}\"'\n",
    "            df = pd.read_sql_query(sql=f\"SELECT * FROM {full_table_name}\", con=engine)\n",
    "            print(f\"Tabla '{database_name}.{schema_name}.{table_name}' leída exitosamente en {time.time() - start_time:.2f} segundos.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer datos de la base de datos: {e}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4ff1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener productos\n",
    "productos = sql_to_df(\n",
    "    database_name = 'OyLCL',\n",
    "    schema_name = 'plan_chile',\n",
    "    table_name = 'productos',\n",
    "    query = 'SELECT * FROM OyLCL.plan_chile.productos',\n",
    "    server_address = \"10.156.16.45\\SQL2019\",\n",
    "    username_sql = username_sql,\n",
    "    password_sql = password_sql\n",
    ")\n",
    "\n",
    "# Transformaciones adicionales\n",
    "productos['cv_id'] = productos['cv_id'].astype('Int64')\n",
    "productos['cv_prod'] = productos['cv_prod'].astype('Int64')\n",
    "productos['cm_kit'] = productos['cm_kit'].astype('Int64')\n",
    "productos['cm_prod'] = productos['cm_prod'].astype('Int64')\n",
    "\n",
    "productos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059e3745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def procesar_demanda_vol(ruta_archivo_vol, df_productos): # Falta validar que sea el mismo código para retail\n",
    "    \"\"\"\n",
    "    Procesa un archivo de demanda de VOL (Venta por Otros Canales) y lo combina\n",
    "    con un DataFrame de productos para calcular la demanda final y estandarizar\n",
    "    los datos.\n",
    "\n",
    "    Args:\n",
    "        ruta_archivo_vol (str): La ruta completa al archivo Excel (.xlsx) de demanda de VOL.\n",
    "        df_productos (pd.DataFrame): Un DataFrame de pandas que contiene la\n",
    "                                      información de los productos, incluyendo\n",
    "                                      la columna 'cv_id' para el join.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un DataFrame procesado con la demanda estandarizada de VOL,\n",
    "                      incluyendo las columnas 'Año', 'Ciclo', 'Subciclo',\n",
    "                      'Código_Kit', 'Código_Producto', 'Canal' y 'Demanda'.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Obtener los datos de demanda de VOL\n",
    "    # Se asume que 'crear_dataframe' es una función existente que carga el Excel.\n",
    "    # Si no, se puede reemplazar por pd.read_excel(ruta_archivo_vol)\n",
    "    try:\n",
    "        exp_vol = pd.read_excel(ruta_archivo_vol)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: El archivo no se encontró en la ruta: {ruta_archivo_vol}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo Excel: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 2. Aislar el número del código de producto\n",
    "    # Se reemplaza 'P' y se convierte a tipo entero de 64 bits para asegurar compatibilidad.\n",
    "    exp_vol['Código'] = exp_vol['Código'].str.replace('P', '', regex=False).astype(np.int64)\n",
    "\n",
    "    # 3. LEFT JOIN con tabla productos\n",
    "    # Combina los DataFrames 'exp_vol' y 'df_productos' usando 'Código' y 'cv_id'.\n",
    "    # Si 'df_productos' no tiene 'cv_id', se generaría un error.\n",
    "    if 'cv_id' not in df_productos.columns:\n",
    "        print(\"Error: El DataFrame de productos debe contener la columna 'cv_id'.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    vol_explotado = pd.merge(exp_vol, df_productos, left_on='Código', right_on='cv_id', how='left')\n",
    "\n",
    "    # Calcula la nueva demanda multiplicando la cantidad de ítems por la cantidad del producto.\n",
    "    vol_explotado['Demanda'] = vol_explotado['Quantidade_Itens'] * vol_explotado['cantidad']\n",
    "\n",
    "    # 4. Filtrar duplicados a nivel de 'cv_prod'\n",
    "    # Elimina filas duplicadas basándose en las columnas especificadas,\n",
    "    # manteniendo solo la primera ocurrencia.\n",
    "    vol_explotado_final = vol_explotado.drop_duplicates(\n",
    "        subset=['Año', 'Ciclo', 'Subciclo', 'Código', 'cv_prod'], keep='first'\n",
    "    )\n",
    "\n",
    "    # 5. Procesamiento de estandarización\n",
    "    # Asigna el valor 'VOL' a la nueva columna 'Canal'.\n",
    "    vol_explotado_final['Canal'] = 'VOL'\n",
    "\n",
    "    # Selecciona y renombra las columnas finales para el DataFrame de salida.\n",
    "    df_demanda_vol = vol_explotado_final[[\n",
    "        'Año',\n",
    "        'Ciclo',\n",
    "        'Subciclo',\n",
    "        'cv_id',\n",
    "        'cv_prod',\n",
    "        'Canal',\n",
    "        'Demanda'\n",
    "    ]].rename(columns={'cv_id': 'Código_Kit', 'cv_prod': 'Código_Producto'})\n",
    "\n",
    "    # Estandariza los campos de 'Año' y 'Ciclo'.\n",
    "    # El año se convierte a un formato de cuatro dígitos (ej., 'FY23' a '2023').\n",
    "    df_demanda_vol['Año'] = df_demanda_vol['Año'].astype(str).str.replace('FY', '', regex=False).astype(int) + 2000\n",
    "    # El ciclo se convierte a un entero sin ceros iniciales (ej., 'C01' a '1').\n",
    "    df_demanda_vol['Ciclo'] = df_demanda_vol['Ciclo'].astype(str).str.replace('C', '', regex=False).str.lstrip('0').astype(int)\n",
    "\n",
    "    return df_demanda_vol\n",
    "\n",
    "def procesar_demanda_retail(ruta_archivo_vol, df_productos): # Falta validar que sea el mismo código para retail\n",
    "    \"\"\"\n",
    "    Procesa un archivo de demanda de RETAIL (Venta por Otros Canales) y lo combina\n",
    "    con un DataFrame de productos para calcular la demanda final y estandarizar\n",
    "    los datos.\n",
    "\n",
    "    Args:\n",
    "        ruta_archivo_vol (str): La ruta completa al archivo Excel (.xlsx) de demanda de RETAIL.\n",
    "        df_productos (pd.DataFrame): Un DataFrame de pandas que contiene la\n",
    "                                      información de los productos, incluyendo\n",
    "                                      la columna 'cv_id' para el join.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un DataFrame procesado con la demanda estandarizada de RETAIL,\n",
    "                      incluyendo las columnas 'Año', 'Ciclo', 'Subciclo',\n",
    "                      'Código_Kit', 'Código_Producto', 'Canal' y 'Demanda'.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Obtener los datos de demanda de RETAIL\n",
    "    # Se asume que 'crear_dataframe' es una función existente que carga el Excel.\n",
    "    # Si no, se puede reemplazar por pd.read_excel(ruta_archivo_vol)\n",
    "    try:\n",
    "        exp_vol = pd.read_excel(ruta_archivo_vol)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: El archivo no se encontró en la ruta: {ruta_archivo_vol}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo Excel: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 2. Aislar el número del código de producto\n",
    "    # Se reemplaza 'P' y se convierte a tipo entero de 64 bits para asegurar compatibilidad.\n",
    "    exp_vol['Código'] = exp_vol['Código'].str.replace('P', '', regex=False).astype(np.int64)\n",
    "\n",
    "    # 3. LEFT JOIN con tabla productos\n",
    "    # Combina los DataFrames 'exp_vol' y 'df_productos' usando 'Código' y 'cv_id'.\n",
    "    # Si 'df_productos' no tiene 'cv_id', se generaría un error.\n",
    "    if 'cv_id' not in df_productos.columns:\n",
    "        print(\"Error: El DataFrame de productos debe contener la columna 'cv_id'.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    vol_explotado = pd.merge(exp_vol, df_productos, left_on='Código', right_on='cv_id', how='left')\n",
    "\n",
    "    # Calcula la nueva demanda multiplicando la cantidad de ítems por la cantidad del producto.\n",
    "    vol_explotado['Demanda'] = vol_explotado['Quantidade_Itens'] * vol_explotado['cantidad']\n",
    "\n",
    "    # 4. Filtrar duplicados a nivel de 'cv_prod'\n",
    "    # Elimina filas duplicadas basándose en las columnas especificadas,\n",
    "    # manteniendo solo la primera ocurrencia.\n",
    "    vol_explotado_final = vol_explotado.drop_duplicates(\n",
    "        subset=['Año', 'Ciclo', 'Subciclo', 'Código', 'cv_prod'], keep='first'\n",
    "    )\n",
    "\n",
    "    # 5. Procesamiento de estandarización\n",
    "    # Asigna el valor 'VOL' a la nueva columna 'Canal'.\n",
    "    vol_explotado_final['Canal'] = 'RETAIL'\n",
    "\n",
    "    # Selecciona y renombra las columnas finales para el DataFrame de salida.\n",
    "    df_demanda_vol = vol_explotado_final[[\n",
    "        'Año',\n",
    "        'Ciclo',\n",
    "        'Subciclo',\n",
    "        'cv_id',\n",
    "        'cv_prod',\n",
    "        'Canal',\n",
    "        'Demanda'\n",
    "    ]].rename(columns={'cv_id': 'Código_Kit', 'cv_prod': 'Código_Producto'})\n",
    "\n",
    "    # Estandariza los campos de 'Año' y 'Ciclo'.\n",
    "    # El año se convierte a un formato de cuatro dígitos (ej., 'FY23' a '2023').\n",
    "    df_demanda_vol['Año'] = df_demanda_vol['Año'].astype(str).str.replace('FY', '', regex=False).astype(int) + 2000\n",
    "    # El ciclo se convierte a un entero sin ceros iniciales (ej., 'C01' a '1').\n",
    "    df_demanda_vol['Ciclo'] = df_demanda_vol['Ciclo'].astype(str).str.replace('C', '', regex=False).str.lstrip('0').astype(int)\n",
    "\n",
    "    return df_demanda_vol\n",
    "\n",
    "def procesar_demanda_vd(ruta_archivo_vd, df_productos):\n",
    "    \"\"\"\n",
    "    Procesa un archivo de demanda de VD (Venta Directa) y lo combina\n",
    "    con un DataFrame de productos para calcular la demanda final y estandarizar\n",
    "    los datos.\n",
    "\n",
    "    Args:\n",
    "        ruta_archivo_vd (str): La ruta completa al archivo Excel (.xlsx) de demanda de VD.\n",
    "        df_productos (pd.DataFrame): Un DataFrame de pandas que contiene la\n",
    "                                      información de los productos, incluyendo\n",
    "                                      la columna 'cv_id' para el join.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un DataFrame procesado con la demanda estandarizada de VD,\n",
    "                      incluyendo las columnas 'Año', 'Ciclo', 'Subciclo',\n",
    "                      'Código_Kit', 'Código_Producto', 'Canal' y 'Demanda'.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Obtener los datos de demanda de VD\n",
    "    # Se asume que 'crear_dataframe' es una función existente que carga el Excel.\n",
    "    # Si no, se puede reemplazar por pd.read_excel(ruta_archivo_vd)\n",
    "    try:\n",
    "        exp_vd = pd.read_excel(ruta_archivo_vd)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: El archivo no se encontró en la ruta: {ruta_archivo_vd}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo Excel: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 2. Aislar el número del código de producto\n",
    "    # Se extraen los dígitos del código y se convierten a Int64 para manejar valores nulos.\n",
    "    exp_vd['Cód - Descripción'] = exp_vd['Cód - Descripción'].astype(str).str.extract(r'(\\d+)')[0].astype('Int64')\n",
    "\n",
    "    # 3. LEFT JOIN con tabla productos\n",
    "    # Se verifica si 'cv_id' existe en el DataFrame de productos antes de intentar el merge.\n",
    "    if 'cv_id' not in df_productos.columns:\n",
    "        print(\"Error: El DataFrame de productos debe contener la columna 'cv_id'.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    vd_explotado = pd.merge(exp_vd, df_productos, left_on='Cód - Descripción', right_on='cv_id', how='left')\n",
    "\n",
    "    # Calculando nueva demanda\n",
    "    vd_explotado['Demanda'] = vd_explotado['Cantidad Itens'] * vd_explotado['cantidad']\n",
    "\n",
    "    # 4. Filtrar duplicados a nivel de 'cv_prod'\n",
    "    # Elimina filas duplicadas basándose en las columnas especificadas,\n",
    "    # manteniendo solo la primera ocurrencia.\n",
    "    vd_explotado_final = vd_explotado.drop_duplicates(\n",
    "        subset=['Año', 'Ciclo', 'Subciclo', 'Cód - Descripción', 'cv_prod'], keep='first'\n",
    "    )\n",
    "\n",
    "    # 5. Procesamiento de estandarización\n",
    "    # Asigna el valor 'VD' a la nueva columna 'Canal'.\n",
    "    vd_explotado_final['Canal'] = 'VD'\n",
    "\n",
    "    # Selecciona y renombra las columnas finales para el DataFrame de salida.\n",
    "    df_demanda_vd = vd_explotado_final[[\n",
    "        'Año',\n",
    "        'Ciclo',\n",
    "        'Subciclo',\n",
    "        'cv_id',\n",
    "        'cv_prod',\n",
    "        'Canal',\n",
    "        'Demanda'\n",
    "    ]].rename(columns={'cv_id': 'Código_Kit', 'cv_prod': 'Código_Producto'})\n",
    "\n",
    "    # Estandariza los campos de 'Ciclo' y 'Subciclo'.\n",
    "    # El ciclo se convierte a un entero sin el prefijo 'Ciclo '.\n",
    "    df_demanda_vd['Ciclo'] = df_demanda_vd['Ciclo'].astype(str).str.replace('Ciclo ', '', regex=False).str.lstrip('0').astype(int)\n",
    "    # El subciclo se limpia del prefijo 'Subciclo '.\n",
    "    df_demanda_vd['Subciclo'] = df_demanda_vd['Subciclo'].astype(str).str.replace('Subciclo ', '', regex=False)\n",
    "    # Asegura que la demanda sea de tipo entero.\n",
    "    df_demanda_vd['Demanda'] = df_demanda_vd['Demanda'].astype('Int64')\n",
    "\n",
    "    return df_demanda_vd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f9fedf",
   "metadata": {},
   "source": [
    "## Distribución de demandas explotadas en curva estandar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b37372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versión 2 final\n",
    "import pandas as pd\n",
    "\n",
    "def distribuir_demanda(df_demanda_retail, ruta_curva_demanda): # Sirve para vol y retail\n",
    "    \"\"\"\n",
    "    Distribuye la demanda del dataframe df_demanda_retail en los días del ciclo\n",
    "    según los porcentajes definidos en la curva de demanda.\n",
    "    Maneja valores de porcentaje con coma decimal y símbolo '%'.\n",
    "    Estandariza los campos 'Año' y 'Ciclo' en df_curva para que coincidan con df_demanda_retail.\n",
    "    Mantiene los nombres originales de las columnas de días (ej. 'Día 1', 'Día 2').\n",
    "\n",
    "    Args:\n",
    "        df_demanda_retail (pd.DataFrame): DataFrame con los campos Año, Ciclo, Subciclo,\n",
    "                                        Código_Kit, Código_Producto, Canal y Demanda.\n",
    "        ruta_curva_demanda (str): Ruta al archivo Excel (.xlsx) o CSV que contiene la curva de demanda.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con los datos de df_demanda_retail y las columnas 'Día N'\n",
    "                      que contienen la demanda distribuida.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Cargar la curva de demanda.\n",
    "        if ruta_curva_demanda.endswith('.xlsx'):\n",
    "            df_curva = pd.read_excel(ruta_curva_demanda)\n",
    "        elif ruta_curva_demanda.endswith('.csv'):\n",
    "            df_curva = pd.read_csv(ruta_curva_demanda, decimal=',')\n",
    "        else:\n",
    "            raise ValueError(\"Formato de archivo no soportado. Por favor, use .xlsx o .csv\")\n",
    "\n",
    "        # Identificar las columnas que representan los días en la curva de demanda\n",
    "        columnas_dias_curva = [col for col in df_curva.columns if 'Día' in col]\n",
    "\n",
    "        if not columnas_dias_curva:\n",
    "            raise ValueError(\"No se encontraron columnas de días en la curva de demanda. \"\n",
    "                             \"Asegúrese de que los nombres de las columnas contengan 'Día'.\")\n",
    "\n",
    "        # --- Estandarización de 'Año' y 'Ciclo' en df_curva ---\n",
    "        if 'Año' in df_curva.columns:\n",
    "            df_curva['Año_str_temp'] = df_curva['Año'].astype(str).str.replace('FY', '', regex=False)\n",
    "            df_curva['Año_temp'] = pd.to_numeric(df_curva['Año_str_temp'], errors='coerce')\n",
    "            df_curva['Año'] = df_curva.apply(\n",
    "                lambda row: int(row['Año_temp']) + 2000 if pd.notna(row['Año_temp']) and len(str(int(row['Año_temp']))) <= 2 else int(row['Año']),\n",
    "                axis=1\n",
    "            )\n",
    "            df_curva.drop(columns=['Año_str_temp', 'Año_temp'], inplace=True, errors='ignore')\n",
    "            df_curva['Año'] = df_curva['Año'].astype(int)\n",
    "\n",
    "        if 'Ciclo' in df_curva.columns:\n",
    "            df_curva['Ciclo'] = df_curva['Ciclo'].astype(str).str.replace('C', '', regex=False).str.lstrip('0')\n",
    "            df_curva['Ciclo'] = pd.to_numeric(df_curva['Ciclo'], errors='coerce').fillna(0).astype(int)\n",
    "        # --- FIN Estandarización ---\n",
    "\n",
    "        # --- APLICAR LA LIMPIEZA Y CONVERSIÓN DE PORCENTAJES ---\n",
    "        for col in columnas_dias_curva:\n",
    "            df_curva[col] = (\n",
    "                df_curva[col]\n",
    "                .astype(str)\n",
    "                .str.replace('%', '', regex=False)\n",
    "                .str.replace(',', '.', regex=False)\n",
    "            )\n",
    "            df_curva[col] = pd.to_numeric(df_curva[col], errors='coerce')\n",
    "        # --- FIN Limpieza de Porcentajes ---\n",
    "\n",
    "        # --- Preparar el DataFrame de resultado ---\n",
    "        # Obtener el conjunto único de todas las columnas 'Día N' que existen en df_curva\n",
    "        all_day_columns = sorted([col for col in df_curva.columns if 'Día' in col and col.replace('Día ', '').isdigit()],\n",
    "                                 key=lambda x: int(x.replace('Día ', '')))\n",
    "        \n",
    "        # Crear un df temporal con las columnas de demanda retail + todas las columnas de días inicializadas en 0\n",
    "        df_resultado_base = df_demanda_retail.copy()\n",
    "        for col_dia in all_day_columns:\n",
    "            df_resultado_base[col_dia] = 0.0 # Inicializar con float\n",
    "\n",
    "\n",
    "        # Iterar sobre cada fila de df_demanda_retail (que ahora es df_resultado_base)\n",
    "        for index, row in df_resultado_base.iterrows():\n",
    "            # Asegurarse de que 'Año', 'Ciclo', 'Subciclo' de df_demanda_retail sean int para la comparación\n",
    "            año = row['Año']\n",
    "            ciclo = row['Ciclo']\n",
    "            subciclo = row['Subciclo']\n",
    "            demanda_total = row['Demanda']\n",
    "\n",
    "            # Buscar la curva de demanda correspondiente a Año, Ciclo y Subciclo\n",
    "            curva_especifica = df_curva[\n",
    "                (df_curva['Año'] == año) &\n",
    "                (df_curva['Ciclo'] == ciclo) &\n",
    "                (df_curva['Subciclo'] == subciclo)\n",
    "            ]\n",
    "\n",
    "            if not curva_especifica.empty:\n",
    "                curva_especifica = curva_especifica.iloc[0] # Tomar la primera coincidencia\n",
    "                \n",
    "                # Para cada columna de día identificada en la curva de demanda\n",
    "                for col_dia_name in columnas_dias_curva:\n",
    "                    try:\n",
    "                        valor_porcentaje_curva = curva_especifica[col_dia_name]\n",
    "                        porcentaje_dia = valor_porcentaje_curva if pd.notna(valor_porcentaje_curva) else 0\n",
    "                        demanda_distribuida = porcentaje_dia\n",
    "\n",
    "                        # Asignar la demanda distribuida directamente a la columna 'Día N'\n",
    "                        if col_dia_name in df_resultado_base.columns:\n",
    "                            df_resultado_base.at[index, col_dia_name] = demanda_distribuida\n",
    "                        # else: # No es necesario un else aquí, ya que se inicializan todas las columnas\n",
    "                        #     pass # Si la columna no existe, no se hace nada y se queda con el 0 inicial\n",
    "\n",
    "                    except KeyError:\n",
    "                        # Esto ocurriría si col_dia_name existe en columnas_dias_curva pero no en curva_especifica.\n",
    "                        # Dado cómo se construye columnas_dias_curva, es poco probable para un 'Día N' válido.\n",
    "                        # Se mantendría el 0 inicializado en df_resultado_base.\n",
    "                        pass\n",
    "                    except Exception:\n",
    "                        # Cualquier otro error durante el cálculo de un día, mantiene el 0 inicializado\n",
    "                        pass\n",
    "\n",
    "            # else: # No es necesario un else aquí, las columnas de días ya están en 0 en df_resultado_base\n",
    "                # Si no se encuentra curva, las columnas de días ya están inicializadas a 0\n",
    "                # pass\n",
    "\n",
    "        return df_resultado_base # Retornar el DataFrame base modificado\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: El archivo no fue encontrado en la ruta '{ruta_curva_demanda}'.\")\n",
    "        return pd.DataFrame()\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Falta una columna esperada en uno de los DataFrames. Detalle: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error inesperado: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1583d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def transformar_df_dias(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma un DataFrame de Pandas, convirtiendo columnas de días\n",
    "    ('Día 1', 'Día 2', ..., 'Día N') en una sola columna 'Dia_Ciclo'\n",
    "    y los valores correspondientes en una columna 'Valor_Dia_Ciclo'.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): El DataFrame original con las columnas de días.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: El DataFrame transformado con una columna 'Dia_Ciclo'\n",
    "                      y los valores de demanda por día en 'Valor_Dia_Ciclo'.\n",
    "    \"\"\"\n",
    "    # Identifica las columnas que contienen los datos de los días\n",
    "    columnas_dias = [col for col in df.columns if col.startswith('Día ')]\n",
    "\n",
    "    # Identifica las columnas que no se van a \"derretir\" (mantener como identificadores)\n",
    "    columnas_id = ['Año', 'Ciclo', 'Subciclo', 'Código_Kit', 'Código_Producto', 'Canal', 'Demanda']\n",
    "\n",
    "    # Utiliza pd.melt para \"despivotar\" el DataFrame\n",
    "    df_transformado = pd.melt(df,\n",
    "                              id_vars=columnas_id,\n",
    "                              value_vars=columnas_dias,\n",
    "                              var_name='Dia_Ciclo',\n",
    "                              value_name='Valor_Dia_Ciclo') # ¡Aquí está el cambio!\n",
    "\n",
    "    # Extraer el número del día de la columna 'Dia_Ciclo' (ej. 'Día 1' -> 1)\n",
    "    # y convertirlo a tipo entero para facilitar operaciones futuras\n",
    "    df_transformado['Dia_Ciclo'] = df_transformado['Dia_Ciclo'].str.replace('Día ', '').astype(int)\n",
    "\n",
    "    # Reordenar las columnas para que 'Dia_Ciclo' y 'Valor_Dia_Ciclo'\n",
    "    # estén en el orden deseado\n",
    "    orden_columnas = ['Año', 'Ciclo', 'Subciclo', 'Código_Kit', 'Código_Producto',\n",
    "                      'Canal', 'Demanda', 'Dia_Ciclo', 'Valor_Dia_Ciclo']\n",
    "    df_transformado = df_transformado[orden_columnas]\n",
    "\n",
    "    return df_transformado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc00ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este método funciona para clasificar los días del ciclo calendario\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "def incorporar_dia_calendario(\n",
    "    demanda_df: pd.DataFrame,\n",
    "    ruta_calendario_excel: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Incorpora el Dia_Calendario calculado al DataFrame de demanda_retail_curva,\n",
    "    leyendo el calendario desde un archivo Excel (.xlsx) y determinando la fecha\n",
    "    basada en el Dia_Ciclo dentro del rango Fecha Inicio y Fecha Fin.\n",
    "    Asume que el archivo Excel del calendario tiene una sola hoja.\n",
    "\n",
    "    Args:\n",
    "        demanda_df (pd.DataFrame): DataFrame 'demanda_retail_curva' con columnas\n",
    "                                   Año, Ciclo, Subciclo, Dia_Ciclo.\n",
    "                                   (Asumiendo 'Año' en formato 'FYxx', 'Ciclo' en 'Cxx', 'Subciclo' en 'A/B/etc.')\n",
    "        ruta_calendario_excel (str): La ruta completa al archivo Excel del calendario.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un nuevo DataFrame con Dia_Calendario, Dia_Inicio y Dia_Fin\n",
    "                      incorporados, en formato dd-mm-yyyy.\n",
    "    \"\"\"\n",
    "    print(\"--- DEBUG: Paso 1 - Inicio de la función ---\")\n",
    "    print(\"demanda_df original head:\\n\", demanda_df.head())\n",
    "    print(\"demanda_df dtypes:\\n\", demanda_df.dtypes)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    try:\n",
    "        calendario_df_raw = pd.read_excel(ruta_calendario_excel)\n",
    "        print(\"--- DEBUG: Paso 2 - calendario_df_raw cargado ---\")\n",
    "        print(\"calendario_df_raw head:\\n\", calendario_df_raw.head())\n",
    "        print(\"calendario_df_raw dtypes:\\n\", calendario_df_raw.dtypes)\n",
    "        print(\"\\n\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: El archivo Excel no se encontró en la ruta: {ruta_calendario_excel}\")\n",
    "        return demanda_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo Excel del calendario: {e}\")\n",
    "        return demanda_df\n",
    "\n",
    "    # --- Preprocesamiento del calendario_df ---\n",
    "    calendario_df = calendario_df_raw.copy()\n",
    "\n",
    "    # Convertir 'Año'\n",
    "    if 'Año' in calendario_df.columns and calendario_df['Año'].dtype == object and \\\n",
    "       calendario_df['Año'].astype(str).str.startswith('FY').any():\n",
    "        calendario_df['Año'] = calendario_df['Año'].astype(str).str.replace('FY', '20').astype(int)\n",
    "    else:\n",
    "        calendario_df['Año'] = calendario_df['Año'].astype(int)\n",
    "\n",
    "    # Convertir 'Ciclo'\n",
    "    if 'Ciclo' in calendario_df.columns and calendario_df['Ciclo'].dtype == object and \\\n",
    "       calendario_df['Ciclo'].astype(str).str.startswith('C').any():\n",
    "        calendario_df['Ciclo'] = calendario_df['Ciclo'].astype(str).str.replace('C', '').astype(int)\n",
    "    else:\n",
    "        calendario_df['Ciclo'] = calendario_df['Ciclo'].astype(int)\n",
    "\n",
    "    # Asegurarse de que 'Subciclo' sea string y limpiar espacios\n",
    "    if 'Subciclo' in calendario_df.columns:\n",
    "        calendario_df['Subciclo'] = calendario_df['Subciclo'].astype(str).str.strip()\n",
    "\n",
    "    # Convertir 'Fecha Inicio' y 'Fecha Fin' a tipo datetime\n",
    "    calendario_df['Fecha Inicio'] = pd.to_datetime(calendario_df['Fecha Inicio'], errors='coerce')\n",
    "    calendario_df['Fecha Fin'] = pd.to_datetime(calendario_df['Fecha Fin'], errors='coerce')\n",
    "\n",
    "    # Calcular la duración del ciclo en días (inclusive)\n",
    "    calendario_df['Duracion_Real_Dias'] = (calendario_df['Fecha Fin'] - calendario_df['Fecha Inicio']).dt.days + 1\n",
    "\n",
    "    print(\"--- DEBUG: Paso 3 - calendario_df preprocesado ---\")\n",
    "    print(\"calendario_df head:\\n\", calendario_df.head())\n",
    "    print(\"calendario_df dtypes:\\n\", calendario_df.dtypes)\n",
    "    # Verificar NaT en fechas\n",
    "    print(\"NaT en Fecha Inicio (calendario_df):\", calendario_df['Fecha Inicio'].isna().sum())\n",
    "    print(\"NaT en Fecha Fin (calendario_df):\", calendario_df['Fecha Fin'].isna().sum())\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "    calendario_info = calendario_df[['Año', 'Ciclo', 'Subciclo', 'Fecha Inicio', 'Fecha Fin', 'Duracion_Real_Dias']].copy()\n",
    "\n",
    "    demanda_df_procesado = demanda_df.copy()\n",
    "\n",
    "    # Asegurar que las columnas de fusión en demanda_df_procesado tengan el mismo tipo\n",
    "    demanda_df_procesado['Año'] = demanda_df_procesado['Año'].astype(int)\n",
    "    demanda_df_procesado['Ciclo'] = demanda_df_procesado['Ciclo'].astype(int)\n",
    "    demanda_df_procesado['Subciclo'] = demanda_df_procesado['Subciclo'].astype(str).str.strip()\n",
    "\n",
    "\n",
    "    print(\"--- DEBUG: Paso 4 - demanda_df_procesado antes de la fusión ---\")\n",
    "    print(\"demanda_df_procesado head:\\n\", demanda_df_procesado.head())\n",
    "    print(\"demanda_df_procesado dtypes:\\n\", demanda_df_procesado.dtypes)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Fusionar los DataFrames para obtener las fechas de inicio y fin para cada fila de demanda\n",
    "    demanda_con_fechas_ciclo = pd.merge(\n",
    "        demanda_df_procesado,\n",
    "        calendario_info,\n",
    "        on=['Año', 'Ciclo', 'Subciclo'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    print(\"--- DEBUG: Paso 5 - demanda_con_fechas_ciclo después de la fusión ---\")\n",
    "    print(\"demanda_con_fechas_ciclo head:\\n\", demanda_con_fechas_ciclo.head(10)) # Muestra más filas\n",
    "    print(\"demanda_con_fechas_ciclo dtypes:\\n\", demanda_con_fechas_ciclo.dtypes)\n",
    "    # Verificar cuántas filas tienen NaN en Fecha Inicio o Fecha Fin después de la fusión\n",
    "    print(\"Filas con NaN en 'Fecha Inicio' o 'Fecha Fin' después de fusión:\",\n",
    "          demanda_con_fechas_ciclo[['Fecha Inicio', 'Fecha Fin', 'Duracion_Real_Dias']].isna().any(axis=1).sum())\n",
    "    # Muestra las filas que no encontraron match\n",
    "    print(\"Filas sin match en calendario:\\n\", demanda_con_fechas_ciclo[demanda_con_fechas_ciclo['Fecha Inicio'].isna()])\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Función para calcular Dia_Calendario, Dia_Inicio y Dia_Fin\n",
    "    def calcular_fechas_para_fila(row):\n",
    "        fecha_inicio_ciclo = row['Fecha Inicio']\n",
    "        fecha_fin_ciclo = row['Fecha Fin']\n",
    "        dia_ciclo_demanda = row['Dia_Ciclo']\n",
    "        duracion_ciclo = row['Duracion_Real_Dias']\n",
    "\n",
    "        dia_calendario = pd.NaT\n",
    "        dia_inicio_formato = None\n",
    "        dia_fin_formato = None\n",
    "\n",
    "        if pd.notna(fecha_inicio_ciclo) and pd.notna(fecha_fin_ciclo) and pd.notna(dia_ciclo_demanda):\n",
    "            # Asegurarse de que dia_ciclo_demanda sea un entero antes de la operación\n",
    "            try:\n",
    "                dia_ciclo_demanda_int = int(dia_ciclo_demanda)\n",
    "            except ValueError:\n",
    "                return pd.Series({\n",
    "                    'Dia_Calendario': pd.NaT,\n",
    "                    'Dia_Inicio': None,\n",
    "                    'Dia_Fin': None\n",
    "                })\n",
    "\n",
    "            calculated_date = fecha_inicio_ciclo + timedelta(days=dia_ciclo_demanda_int - 1)\n",
    "\n",
    "            # Validar rango\n",
    "            if fecha_inicio_ciclo <= calculated_date <= fecha_fin_ciclo and \\\n",
    "               1 <= dia_ciclo_demanda_int <= duracion_ciclo:\n",
    "                dia_calendario = calculated_date\n",
    "            # else: dia_calendario ya es pd.NaT\n",
    "\n",
    "            if pd.notna(fecha_inicio_ciclo):\n",
    "                dia_inicio_formato = fecha_inicio_ciclo.strftime('%d-%m-%Y')\n",
    "            if pd.notna(fecha_fin_ciclo):\n",
    "                dia_fin_formato = fecha_fin_ciclo.strftime('%d-%m-%Y')\n",
    "\n",
    "        return pd.Series({\n",
    "            'Dia_Calendario': dia_calendario,\n",
    "            'Dia_Inicio': dia_inicio_formato,\n",
    "            'Dia_Fin': dia_fin_formato\n",
    "        })\n",
    "\n",
    "    # Aplicar la función a cada fila del DataFrame\n",
    "    fechas_calculadas = demanda_con_fechas_ciclo.apply(calcular_fechas_para_fila, axis=1)\n",
    "\n",
    "    print(\"--- DEBUG: Paso 6 - fechas_calculadas después de apply ---\")\n",
    "    print(\"fechas_calculadas head:\\n\", fechas_calculadas.head(10))\n",
    "    print(\"NaT en Dia_Calendario (fechas_calculadas):\", fechas_calculadas['Dia_Calendario'].isna().sum())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Unir las nuevas columnas al DataFrame original\n",
    "    demanda_retail_curva_calendario = pd.concat([demanda_con_fechas_ciclo, fechas_calculadas], axis=1)\n",
    "\n",
    "    # Eliminar las columnas temporales\n",
    "    demanda_retail_curva_calendario = demanda_retail_curva_calendario.drop(columns=[\n",
    "        'Fecha Inicio', 'Fecha Fin', 'Duracion_Real_Dias'\n",
    "    ])\n",
    "\n",
    "    # Reordenar las columnas\n",
    "    cols = demanda_retail_curva_calendario.columns.tolist()\n",
    "    new_date_cols = ['Dia_Calendario', 'Dia_Inicio', 'Dia_Fin']\n",
    "    existing_new_date_cols = [col for col in new_date_cols if col in cols]\n",
    "    for col in existing_new_date_cols:\n",
    "        if col in cols: # Comprobar de nuevo por si se eliminó en la iteración anterior (no debería ocurrir aquí)\n",
    "            cols.remove(col)\n",
    "\n",
    "    if 'Dia_Ciclo' in cols:\n",
    "        idx_dia_ciclo = cols.index('Dia_Ciclo')\n",
    "        for i, col in enumerate(existing_new_date_cols):\n",
    "            cols.insert(idx_dia_ciclo + 1 + i, col)\n",
    "    else:\n",
    "        cols.extend(existing_new_date_cols)\n",
    "\n",
    "    demanda_retail_curva_calendario = demanda_retail_curva_calendario[cols]\n",
    "\n",
    "    # Formatear la columna 'Dia_Calendario'\n",
    "    demanda_retail_curva_calendario['Dia_Calendario'] = \\\n",
    "        demanda_retail_curva_calendario['Dia_Calendario'].dt.strftime('%d-%m-%Y').replace({pd.NaT: None})\n",
    "\n",
    "    print(\"--- DEBUG: Paso 7 - DataFrame final (primeras 10 filas) ---\")\n",
    "    print(demanda_retail_curva_calendario.head(10))\n",
    "    print(\"NaT/None en Dia_Calendario (final):\", demanda_retail_curva_calendario['Dia_Calendario'].isna().sum())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return demanda_retail_curva_calendario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17faa7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versión de ajustada con condiciones de borde // FUNCIONA BIEN DEJAR\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def calcular_demanda_por_dia(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calcula las columnas 'Demanda_Dia_Ciclo', 'Demanda_Dia_Calendario' y 'Valor_Dia_Ciclo_Calendario'\n",
    "    en el DataFrame, utilizando la fecha actual del sistema.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame de entrada con la información de demanda.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: El DataFrame con las nuevas columnas calculadas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtener la fecha de hoy y convertirla a un Timestamp de Pandas\n",
    "    fecha_hoy = pd.to_datetime(datetime.now().date())\n",
    "    print(f\"La fecha actual considerada para el cálculo es: {fecha_hoy.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "    # Asegúrate de que las columnas de fecha sean tipo datetime y con el formato correcto\n",
    "    df['Dia_Inicio'] = pd.to_datetime(df['Dia_Inicio'], format='%d-%m-%Y', errors='coerce')\n",
    "    df['Dia_Fin'] = pd.to_datetime(df['Dia_Fin'], format='%d-%m-%Y', errors='coerce')\n",
    "    df['Dia_Calendario'] = pd.to_datetime(df['Dia_Calendario'], format='%d-%m-%Y', errors='coerce')\n",
    "\n",
    "    # 1. Calcular Demanda_Dia_Ciclo (Esto se mantiene igual)\n",
    "    df['Demanda_Dia_Ciclo'] = df['Demanda'] * df['Valor_Dia_Ciclo']\n",
    "\n",
    "    # 2. Inicializar Demanda_Dia_Calendario con Demanda_Dia_Ciclo y Valor_Dia_Ciclo_Calendario con Valor_Dia_Ciclo\n",
    "    df['Demanda_Dia_Calendario'] = df['Demanda_Dia_Ciclo']\n",
    "    df['Valor_Dia_Ciclo_Calendario'] = df['Valor_Dia_Ciclo'] # Inicializar con el valor original\n",
    "\n",
    "    # Función auxiliar para aplicar la lógica de cálculo por grupo\n",
    "    def _calcular_demanda_grupo(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        group = group.copy()\n",
    "\n",
    "        dia_inicio_grupo = group['Dia_Inicio'].iloc[0]\n",
    "        dia_fin_grupo = group['Dia_Fin'].iloc[0]\n",
    "        demanda_total_ciclo = group['Demanda'].iloc[0]\n",
    "\n",
    "        # Condición principal: Si fecha_hoy está dentro del rango del ciclo [Dia_Inicio, Dia_Fin]\n",
    "        if fecha_hoy >= dia_inicio_grupo and fecha_hoy <= dia_fin_grupo:\n",
    "            # Los días hasta e incluyendo fecha_hoy mantienen Demanda_Dia_Ciclo como Demanda_Dia_Calendario\n",
    "            mask_dias_hasta_hoy = (group['Dia_Calendario'] <= fecha_hoy)\n",
    "            group.loc[mask_dias_hasta_hoy, 'Demanda_Dia_Calendario'] = group.loc[mask_dias_hasta_hoy, 'Demanda_Dia_Ciclo']\n",
    "            group.loc[mask_dias_hasta_hoy, 'Valor_Dia_Ciclo_Calendario'] = group.loc[mask_dias_hasta_hoy, 'Valor_Dia_Ciclo']\n",
    "\n",
    "            # Suma de la demanda hasta fecha_hoy (inclusive)\n",
    "            demanda_consumida_hasta_hoy = group.loc[mask_dias_hasta_hoy, 'Demanda_Dia_Calendario'].sum()\n",
    "\n",
    "            # Caso especial: Si fecha_hoy es el último día del ciclo (fecha_fin)\n",
    "            if fecha_hoy == dia_fin_grupo:\n",
    "                # No hay demanda pendiente para distribuir, se habrá \"cumplido\" el 100% de la demanda hasta hoy.\n",
    "                # Ya los valores de Demanda_Dia_Calendario y Valor_Dia_Ciclo_Calendario fueron establecidos arriba.\n",
    "                pass\n",
    "            else:\n",
    "                # Calcular la demanda pendiente a distribuir (desde mañana hasta fecha_fin)\n",
    "                demanda_pendiente_a_distribuir = demanda_total_ciclo - demanda_consumida_hasta_hoy\n",
    "                \n",
    "                # Asegurarse de que la demanda pendiente no sea negativa\n",
    "                demanda_pendiente_a_distribuir = max(0, demanda_pendiente_a_distribuir)\n",
    "\n",
    "                # Días futuros: desde fecha_hoy + 1 hasta fecha_fin\n",
    "                fecha_manana = fecha_hoy + timedelta(days=1)\n",
    "                mask_dias_futuros = (group['Dia_Calendario'] >= fecha_manana) & \\\n",
    "                                    (group['Dia_Calendario'] <= dia_fin_grupo)\n",
    "\n",
    "                # Suma de los Valor_Dia_Ciclo ORIGINALES para los días futuros\n",
    "                suma_valor_dia_ciclo_futuro_original = group.loc[mask_dias_futuros, 'Valor_Dia_Ciclo'].sum()\n",
    "\n",
    "                if suma_valor_dia_ciclo_futuro_original > 0:\n",
    "                    # Recalcular Valor_Dia_Ciclo_Calendario para los días futuros\n",
    "                    # Los nuevos porcentajes deben sumar 1 (100%) para la demanda pendiente\n",
    "                    recalibrated_valor_dia_ciclo = group.loc[mask_dias_futuros, 'Valor_Dia_Ciclo'] / suma_valor_dia_ciclo_futuro_original\n",
    "                    group.loc[mask_dias_futuros, 'Valor_Dia_Ciclo_Calendario'] = recalibrated_valor_dia_ciclo\n",
    "\n",
    "                    # Distribuir la demanda pendiente\n",
    "                    group.loc[mask_dias_futuros, 'Demanda_Dia_Calendario'] = demanda_pendiente_a_distribuir * recalibrated_valor_dia_ciclo\n",
    "                else:\n",
    "                    # Si no hay días futuros con Valor_Dia_Ciclo > 0 para redistribuir,\n",
    "                    # la demanda pendiente no se distribuye. Estos días quedan en 0 si no tienen valor inicial.\n",
    "                    group.loc[mask_dias_futuros, 'Demanda_Dia_Calendario'] = 0\n",
    "                    group.loc[mask_dias_futuros, 'Valor_Dia_Ciclo_Calendario'] = 0\n",
    "        \n",
    "        # Casos donde fecha_hoy está fuera del rango [Dia_Inicio, Dia_Fin]\n",
    "        # (fecha_hoy > dia_fin_grupo o fecha_hoy < dia_inicio_grupo):\n",
    "        # En estos casos, Demanda_Dia_Calendario y Valor_Dia_Ciclo_Calendario\n",
    "        # ya fueron inicializados con los valores de Demanda_Dia_Ciclo y Valor_Dia_Ciclo,\n",
    "        # lo cual es el comportamiento deseado según tus reglas iniciales.\n",
    "\n",
    "        return group\n",
    "\n",
    "    # Aplicar la función por grupo\n",
    "    df_resultado = df.groupby(['Año', 'Ciclo', 'Subciclo', 'Código_Kit', 'Código_Producto', 'Canal'], group_keys=False).apply(_calcular_demanda_grupo)\n",
    "\n",
    "    # Asegurarse de que el orden de las columnas se mantenga o se añadan al final\n",
    "    return df_resultado.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c623d5e",
   "metadata": {},
   "source": [
    "## Procesar todo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0481492",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Obtener demandas\n",
    "df_demanda_retail = procesar_demanda_retail(r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\Demanda Retail.xlsx\", productos)\n",
    "df_demanda_vol = procesar_demanda_vol(r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\Carga de demanda Vol.xlsx\", productos)\n",
    "df_demanda_vd = procesar_demanda_vd(r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\Carga de demanda VD.xlsx\", productos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c563255",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demanda_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01434691",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demanda_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83251c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demanda_vd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7491378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener demandas explotadas con curva sin calendario\n",
    "demanda_retail = distribuir_demanda(df_demanda_retail, r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\Curva Synplin Retail.xlsx\")\n",
    "demanda_vol = distribuir_demanda(df_demanda_vol, r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\Curva Synplin Vol.xlsx\")\n",
    "demanda_vd = distribuir_demanda(df_demanda_vd, r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\Curva Synplin VD.xlsx\")\n",
    "\n",
    "# Concatenar las 3 demandas\n",
    "demanda_curva = pd.concat([demanda_retail, demanda_vol, demanda_vd], ignore_index=True)\n",
    "\n",
    "demanda_curva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a305e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiar el dataframe con campo días distribuido por producto\n",
    "demanda_curva_distribuida = transformar_df_dias(demanda_curva)\n",
    "demanda_curva_distribuida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f033487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clasificar días de ciclos según días calendario\n",
    "demanda_curva_calendario = incorporar_dia_calendario(demanda_curva_distribuida, r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\Calendario Apertura y Cierre 2025.xlsx\")\n",
    "demanda_curva_calendario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43098e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular demanda de cada día calendario, según la fecha de hoy.\n",
    "demanda_dia_calendario_recalculada = calcular_demanda_por_dia(demanda_curva_calendario)\n",
    "\n",
    "demanda_dia_calendario_recalculada.to_excel(r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\demanda_explotada_calendarizada.xlsx\", index = False)\n",
    "\n",
    "demanda_dia_calendario_recalculada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4851d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_deseadas = [\n",
    "    'Año', 'Ciclo', 'Subciclo', 'Código_Kit', 'Código_Producto', 'Canal', 'Demanda',\n",
    "    'Dia_Ciclo', 'Dia_Calendario', 'Dia_Inicio', 'Dia_Fin',\n",
    "    'Valor_Dia_Ciclo', 'Valor_Dia_Ciclo_Calendario',\n",
    "    'Demanda_Dia_Ciclo', 'Demanda_Dia_Calendario'\n",
    "]\n",
    "\n",
    "df_filtrado = demanda_dia_calendario_recalculada[columnas_deseadas].to_excel(r\"C:\\Users\\Spider Build\\Downloads\\Plan Chile\\Demandas\\demanda_explotada_calendarizada_ordenado.xlsx\", index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
